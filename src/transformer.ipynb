{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "from pytorch_pretrained_vit import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lukemelas/PyTorch-Pretrained-ViT/releases/download/0.0.2/B_16_imagenet1k.pth\" to /home/empathy/.cache/torch/hub/checkpoints/B_16_imagenet1k.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef8b5db98ad425ea53aea0652e4fc55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'B_16_imagenet1k'\n",
    "model = ViT(model_name, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/preprocessed_data_split_nona_03_07.pkl', 'rb') as f:\n",
    "    train, val, test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>intensity</th>\n",
       "      <th>statement</th>\n",
       "      <th>repeat</th>\n",
       "      <th>gender</th>\n",
       "      <th>mel</th>\n",
       "      <th>mfcc</th>\n",
       "      <th>chromagram</th>\n",
       "      <th>spec_contrast</th>\n",
       "      <th>tonnetz</th>\n",
       "      <th>filename</th>\n",
       "      <th>mel_pad</th>\n",
       "      <th>mfcc_pad</th>\n",
       "      <th>chromagram_pad</th>\n",
       "      <th>spec_contrast_pad</th>\n",
       "      <th>tonnetz_pad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[-976.11755, -976.11755, -976.11755, -976.117...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[18.698652440717126, 18.698652440717126, 18.6...</td>\n",
       "      <td>[[-0.24351785481696392, -0.22269760507170488, ...</td>\n",
       "      <td>03-01-02-01-02-02-20.wav</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[-976.1175537109375, -976.1175537109375, -976...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[18.698652440717126, 18.698652440717126, 18.6...</td>\n",
       "      <td>[[-0.24351785481696392, -0.22269760507170488, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[-919.54193, -919.54193, -919.54193, -919.541...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[14.30063620727286, 14.30063620727286, 14.300...</td>\n",
       "      <td>[[0.011945099331460431, 0.02484843939140515, 0...</td>\n",
       "      <td>03-01-06-01-01-01-14.wav</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[-919.5419311523438, -919.5419311523438, -919...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[14.30063620727286, 14.30063620727286, 14.300...</td>\n",
       "      <td>[[0.011945099331460431, 0.02484843939140515, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[[5.976571e-14, 3.2825318e-09, 4.7936957e-09, ...</td>\n",
       "      <td>[[-735.2311, -735.2311, -735.2311, -735.2311, ...</td>\n",
       "      <td>[[0.91999584, 0.9419208, 0.9717938, 1.0, 1.0, ...</td>\n",
       "      <td>[[27.160881999404914, 10.240644242524503, 17.3...</td>\n",
       "      <td>[[-0.039178584692475336, -0.07856553551372042,...</td>\n",
       "      <td>03-01-07-01-02-01-04.wav</td>\n",
       "      <td>[[5.976570963388966e-14, 3.282531801929167e-09...</td>\n",
       "      <td>[[-735.2310791015625, -735.2310791015625, -735...</td>\n",
       "      <td>[[0.9199958443641663, 0.941920816898346, 0.971...</td>\n",
       "      <td>[[27.160881999404914, 10.240644242524503, 17.3...</td>\n",
       "      <td>[[-0.039178584692475336, -0.07856553551372042,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[[5.161722e-06, 1.9711595e-06, 1.4549606e-07, ...</td>\n",
       "      <td>[[-760.2454, -761.35565, -763.5334, -763.5334,...</td>\n",
       "      <td>[[0.7726974, 0.86186564, 0.8380048, 0.6996503,...</td>\n",
       "      <td>[[25.352601414654497, 17.07070284635677, 10.08...</td>\n",
       "      <td>[[-0.09442284617849674, -0.08483387511976943, ...</td>\n",
       "      <td>03-01-05-01-02-01-06.wav</td>\n",
       "      <td>[[5.16172212883248e-06, 1.971159463209915e-06,...</td>\n",
       "      <td>[[-760.2454223632812, -761.3556518554688, -763...</td>\n",
       "      <td>[[0.772697389125824, 0.8618656396865845, 0.838...</td>\n",
       "      <td>[[25.352601414654497, 17.07070284635677, 10.08...</td>\n",
       "      <td>[[-0.09442284617849674, -0.08483387511976943, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[[2.1212953e-11, 1.1371189e-10, 2.6217887e-13,...</td>\n",
       "      <td>[[-766.1009, -766.1009, -766.1009, -766.1009, ...</td>\n",
       "      <td>[[0.8027966, 0.9662601, 0.93856305, 0.934313, ...</td>\n",
       "      <td>[[20.23687827597825, 13.043456325271165, 21.71...</td>\n",
       "      <td>[[0.15901379770687685, 0.10643275780873687, 0....</td>\n",
       "      <td>03-01-04-02-02-01-14.wav</td>\n",
       "      <td>[[2.1212953268956447e-11, 1.1371188712860913e-...</td>\n",
       "      <td>[[-766.1008911132812, -766.1008911132812, -766...</td>\n",
       "      <td>[[0.8027966022491455, 0.9662600755691528, 0.93...</td>\n",
       "      <td>[[20.23687827597825, 13.043456325271165, 21.71...</td>\n",
       "      <td>[[0.15901379770687685, 0.10643275780873687, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      emotion  intensity  statement  repeat  gender  \\\n",
       "195         2          1          2       2       0   \n",
       "1048        6          1          1       1       0   \n",
       "532         7          1          2       1       0   \n",
       "1273        5          1          2       1       0   \n",
       "1064        4          2          2       1       0   \n",
       "\n",
       "                                                    mel  \\\n",
       "195   [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1048  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "532   [[5.976571e-14, 3.2825318e-09, 4.7936957e-09, ...   \n",
       "1273  [[5.161722e-06, 1.9711595e-06, 1.4549606e-07, ...   \n",
       "1064  [[2.1212953e-11, 1.1371189e-10, 2.6217887e-13,...   \n",
       "\n",
       "                                                   mfcc  \\\n",
       "195   [[-976.11755, -976.11755, -976.11755, -976.117...   \n",
       "1048  [[-919.54193, -919.54193, -919.54193, -919.541...   \n",
       "532   [[-735.2311, -735.2311, -735.2311, -735.2311, ...   \n",
       "1273  [[-760.2454, -761.35565, -763.5334, -763.5334,...   \n",
       "1064  [[-766.1009, -766.1009, -766.1009, -766.1009, ...   \n",
       "\n",
       "                                             chromagram  \\\n",
       "195   [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1048  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "532   [[0.91999584, 0.9419208, 0.9717938, 1.0, 1.0, ...   \n",
       "1273  [[0.7726974, 0.86186564, 0.8380048, 0.6996503,...   \n",
       "1064  [[0.8027966, 0.9662601, 0.93856305, 0.934313, ...   \n",
       "\n",
       "                                          spec_contrast  \\\n",
       "195   [[18.698652440717126, 18.698652440717126, 18.6...   \n",
       "1048  [[14.30063620727286, 14.30063620727286, 14.300...   \n",
       "532   [[27.160881999404914, 10.240644242524503, 17.3...   \n",
       "1273  [[25.352601414654497, 17.07070284635677, 10.08...   \n",
       "1064  [[20.23687827597825, 13.043456325271165, 21.71...   \n",
       "\n",
       "                                                tonnetz  \\\n",
       "195   [[-0.24351785481696392, -0.22269760507170488, ...   \n",
       "1048  [[0.011945099331460431, 0.02484843939140515, 0...   \n",
       "532   [[-0.039178584692475336, -0.07856553551372042,...   \n",
       "1273  [[-0.09442284617849674, -0.08483387511976943, ...   \n",
       "1064  [[0.15901379770687685, 0.10643275780873687, 0....   \n",
       "\n",
       "                      filename  \\\n",
       "195   03-01-02-01-02-02-20.wav   \n",
       "1048  03-01-06-01-01-01-14.wav   \n",
       "532   03-01-07-01-02-01-04.wav   \n",
       "1273  03-01-05-01-02-01-06.wav   \n",
       "1064  03-01-04-02-02-01-14.wav   \n",
       "\n",
       "                                                mel_pad  \\\n",
       "195   [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1048  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "532   [[5.976570963388966e-14, 3.282531801929167e-09...   \n",
       "1273  [[5.16172212883248e-06, 1.971159463209915e-06,...   \n",
       "1064  [[2.1212953268956447e-11, 1.1371188712860913e-...   \n",
       "\n",
       "                                               mfcc_pad  \\\n",
       "195   [[-976.1175537109375, -976.1175537109375, -976...   \n",
       "1048  [[-919.5419311523438, -919.5419311523438, -919...   \n",
       "532   [[-735.2310791015625, -735.2310791015625, -735...   \n",
       "1273  [[-760.2454223632812, -761.3556518554688, -763...   \n",
       "1064  [[-766.1008911132812, -766.1008911132812, -766...   \n",
       "\n",
       "                                         chromagram_pad  \\\n",
       "195   [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1048  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "532   [[0.9199958443641663, 0.941920816898346, 0.971...   \n",
       "1273  [[0.772697389125824, 0.8618656396865845, 0.838...   \n",
       "1064  [[0.8027966022491455, 0.9662600755691528, 0.93...   \n",
       "\n",
       "                                      spec_contrast_pad  \\\n",
       "195   [[18.698652440717126, 18.698652440717126, 18.6...   \n",
       "1048  [[14.30063620727286, 14.30063620727286, 14.300...   \n",
       "532   [[27.160881999404914, 10.240644242524503, 17.3...   \n",
       "1273  [[25.352601414654497, 17.07070284635677, 10.08...   \n",
       "1064  [[20.23687827597825, 13.043456325271165, 21.71...   \n",
       "\n",
       "                                            tonnetz_pad  \n",
       "195   [[-0.24351785481696392, -0.22269760507170488, ...  \n",
       "1048  [[0.011945099331460431, 0.02484843939140515, 0...  \n",
       "532   [[-0.039178584692475336, -0.07856553551372042,...  \n",
       "1273  [[-0.09442284617849674, -0.08483387511976943, ...  \n",
       "1064  [[0.15901379770687685, 0.10643275780873687, 0....  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 250)\n",
      "(6, 250)\n",
      "(20, 250)\n",
      "(12, 250)\n",
      "(7, 250)\n",
      "(384, 384)\n",
      "1152\n"
     ]
    }
   ],
   "source": [
    "# 256 + 6 + 20 + 12 + 7 = 256 + 45 = 301\n",
    "print(train['mel_pad'][0].shape)\n",
    "print(train['tonnetz_pad'][0].shape)\n",
    "print(train['mfcc_pad'][0].shape)\n",
    "print(train['chromagram_pad'][0].shape)\n",
    "print(train['spec_contrast_pad'][0].shape)\n",
    "print(model.image_size)\n",
    "print(len(train['mel_pad']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# implementing a mel-only version\n",
    "class TransformerDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.labels = df['emotion'].reset_index(drop=True)\n",
    "        self.num_labels = self.labels.nunique()\n",
    "        self.mels = df['mel_pad'].reset_index(drop=True)\n",
    "        self.max_len = self.mels[0].shape[1]\n",
    "        self.transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                             transforms.CenterCrop((self.max_len, self.max_len)),\n",
    "                                             transforms.Resize(model.image_size)])\n",
    "                                             #transforms.Normalize([0.5, 0.5, 0.5],\n",
    "                                                                  #[0.5, 0.5, 0.5]),])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.mels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        mel = self.mels.iloc[idx]\n",
    "        label = self.labels.iloc[idx]        \n",
    "        mel = self.transform(mel)\n",
    "        # stack the same mel spectrogram three times to emulate RGB image\n",
    "        mel = torch.stack([mel]*3, dim=1).squeeze(dim=0).type(torch.float)\n",
    "        label = torch.tensor(label-1).type(torch.long)\n",
    "        \n",
    "        return mel, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1152\n",
      "torch.Size([3, 384, 384])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TransformerDataset(train)\n",
    "print(len(train_dataset))\n",
    "print(train_dataset[0][0].shape)\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "img = train_dataset[0]['mel']\n",
    "with torch.no_grad():\n",
    "    outputs = model(img.float()).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.9006e-01, -4.4152e-01,  1.0690e+00,  8.5133e-01,  4.7515e-01,\n",
      "         1.4316e+00,  6.7732e-01,  1.2937e+00,  1.3502e+00,  4.2589e-01,\n",
      "        -2.1948e-02,  1.8287e-02,  5.3393e-01,  6.2777e-01,  4.0235e-01,\n",
      "         3.8769e-01,  3.0730e-01,  7.2035e-02,  5.7832e-01,  5.7177e-03,\n",
      "         1.4967e-01,  1.1717e+00, -3.1195e-01,  1.0661e+00,  1.1295e+00,\n",
      "         1.1733e-01,  6.9069e-01,  8.8820e-01,  5.2121e-02,  4.0993e-01,\n",
      "         4.1394e-01, -5.7875e-02,  7.2773e-01,  8.5857e-01,  1.4262e+00,\n",
      "         9.1457e-01,  8.6527e-01,  2.7526e-01,  9.3126e-01,  5.0425e-01,\n",
      "         1.1983e-01,  4.9441e-01,  4.8497e-01,  1.0178e+00,  2.4681e-01,\n",
      "         6.2170e-01,  4.5660e-01,  1.0780e+00,  1.7463e-01,  7.5374e-02,\n",
      "        -4.0800e-02,  4.5961e-01,  5.2406e-01,  3.2302e-01,  7.8556e-01,\n",
      "         2.9601e-01,  7.1838e-01, -3.1687e-01,  9.8314e-02,  2.9557e-01,\n",
      "         1.4868e+00,  8.4138e-01,  5.2131e-01, -2.9590e-01,  4.3673e-01,\n",
      "        -2.6020e-01,  8.6630e-01,  3.8989e-01,  9.0892e-01,  3.4975e-01,\n",
      "        -2.8248e-02,  7.2787e-01,  2.7296e-01,  3.5697e-01, -8.7367e-03,\n",
      "        -3.6271e-01,  3.4976e-01, -1.5460e-01,  4.0667e-01,  4.2496e-01,\n",
      "         1.3886e+00,  5.3881e-01,  1.5637e+00,  1.6742e+00,  4.0926e-01,\n",
      "         1.0377e+00,  1.7429e+00,  4.2418e-01, -2.0511e-01,  5.5767e-01,\n",
      "         1.6021e-02,  5.4330e-01,  5.9891e-01,  5.2006e-01, -6.1496e-02,\n",
      "         5.8455e-01,  7.0019e-01,  8.1168e-01,  6.6127e-01,  5.1150e-01,\n",
      "         3.3848e-01, -4.2292e-03, -6.0097e-01, -1.8454e-01, -8.6555e-01,\n",
      "        -4.4988e-01,  3.5165e-01,  3.7479e-01, -2.9845e-02,  2.4758e-01,\n",
      "         6.1417e-01,  5.5816e-01, -5.3879e-01, -1.2842e-01,  3.3657e-01,\n",
      "        -3.3793e-01,  8.1282e-02, -1.8758e-01, -2.9262e-02,  3.9707e-01,\n",
      "         1.2970e-01,  8.0435e-01,  3.2836e-01, -1.5178e-01, -8.3449e-02,\n",
      "         9.9938e-02,  3.7316e-01,  9.1252e-01,  1.2812e+00,  5.5391e-01,\n",
      "         2.8420e-01,  8.7623e-02,  6.3427e-01,  6.0145e-01,  1.2327e+00,\n",
      "         3.7698e-01,  7.1310e-01,  6.9104e-01,  1.2102e+00,  1.8326e-01,\n",
      "         3.2524e-02,  1.6967e-01, -3.0511e-01,  8.3116e-01,  4.8960e-01,\n",
      "         3.7110e-01,  1.4425e+00,  4.9301e-01, -2.3156e-01,  7.4532e-01,\n",
      "        -2.4727e-01, -8.9922e-01,  3.3767e-01, -7.6847e-01, -4.6381e-01,\n",
      "        -1.4593e-01, -8.8453e-01,  2.8676e-01, -2.8378e-01, -4.6313e-02,\n",
      "         2.2586e-02, -7.4660e-01, -8.7902e-01, -2.8054e-01, -4.2045e-01,\n",
      "         1.3766e-01, -2.1796e-01, -7.6101e-02, -2.3278e-01, -1.3907e-01,\n",
      "        -2.6364e-01, -8.0277e-01, -4.1799e-01, -2.0949e-01, -9.5170e-01,\n",
      "         4.4211e-01, -1.4502e-01, -2.6586e-01, -8.2804e-01, -4.8936e-01,\n",
      "        -7.4490e-01,  3.4165e-02, -3.8833e-01, -1.5714e-01,  5.3647e-01,\n",
      "        -2.5090e-01, -4.4967e-01, -1.8955e-01,  2.6730e-01, -2.3989e-01,\n",
      "         6.9237e-02, -1.5947e-02, -7.0697e-01, -2.3482e-01,  2.0649e-01,\n",
      "        -1.0808e+00, -1.1109e-01, -2.5384e-01, -3.5472e-01, -3.5671e-01,\n",
      "        -9.7890e-02, -5.5061e-01, -3.2176e-01, -6.7085e-01, -1.1182e+00,\n",
      "        -4.3040e-01, -2.9393e-01, -4.4871e-01, -7.4932e-01, -2.2016e-01,\n",
      "        -7.1073e-01, -2.5789e-01,  2.3539e-01,  2.7503e-01,  1.7403e-01,\n",
      "        -3.3313e-01,  6.5677e-01, -9.1400e-02, -6.4435e-02, -9.6631e-01,\n",
      "         6.8522e-01,  3.7171e-02, -1.7016e-01, -7.1276e-01,  3.0062e-01,\n",
      "        -4.6825e-01, -1.2797e-01, -2.4944e-01, -4.1002e-01, -3.2317e-01,\n",
      "        -5.3426e-01, -4.7125e-02, -4.4146e-01, -5.6465e-01, -3.8332e-01,\n",
      "        -1.1112e+00, -9.5862e-02, -8.1045e-01, -9.6438e-01, -8.6924e-01,\n",
      "        -1.1301e-01, -4.8610e-01, -1.1401e+00, -7.5867e-01, -7.4753e-01,\n",
      "        -6.7642e-01, -3.4350e-01, -1.1861e-01, -1.4066e-01, -7.7957e-01,\n",
      "        -7.0284e-01, -5.6686e-02, -2.8573e-01, -1.5610e-01, -9.7806e-01,\n",
      "        -5.0802e-01, -6.5954e-01, -3.0195e-01, -3.5452e-01, -8.1912e-01,\n",
      "        -1.0102e+00, -8.3624e-01, -2.2905e-01, -5.4908e-01, -5.9426e-01,\n",
      "        -5.2276e-01, -4.4392e-01, -5.9685e-02, -9.0170e-01, -3.5128e-01,\n",
      "         2.6785e-01,  8.9118e-02, -7.1298e-03, -6.1583e-01,  1.7787e-01,\n",
      "         2.1447e-01,  3.9848e-01,  7.4440e-01,  7.0292e-01,  5.3042e-01,\n",
      "         9.1851e-01, -7.4973e-01, -1.6068e-01,  1.9290e-01, -6.0419e-01,\n",
      "        -1.1744e-01, -1.0811e-01,  1.8943e-01,  2.3239e-01,  1.4389e-01,\n",
      "         1.3575e-01,  4.5905e-01, -2.6223e-01, -2.3963e-01,  6.1531e-01,\n",
      "         3.2078e-02,  6.0324e-01,  5.6908e-01,  1.4815e+00,  9.7518e-01,\n",
      "         3.2499e-01, -6.5452e-02,  6.6679e-01,  1.0860e-01,  2.9929e-02,\n",
      "         4.1098e-01, -4.8655e-01, -7.5454e-01, -8.2113e-02,  2.7506e-01,\n",
      "        -2.6788e-01, -1.6782e-01, -3.5760e-01,  1.1105e-01,  5.7872e-01,\n",
      "        -1.6381e-01, -2.2353e-02,  5.0942e-01,  7.2836e-01,  9.1052e-01,\n",
      "         1.0608e+00,  6.7677e-01,  5.6033e-01,  4.7875e-01,  4.2486e-01,\n",
      "         6.0550e-01,  1.2970e-01,  7.7363e-02,  4.8452e-02,  1.7037e-01,\n",
      "         4.1718e-01,  9.5247e-01,  6.8152e-01,  2.2480e-01,  3.4433e-01,\n",
      "         6.4550e-01,  6.3301e-01,  2.1326e-02,  2.2205e-01, -1.9606e-01,\n",
      "         2.2883e-01,  5.9599e-01,  4.9238e-01,  6.1418e-02,  4.5596e-02,\n",
      "         6.2981e-01, -1.8689e-01, -3.3404e-02,  1.2938e-01,  4.2967e-01,\n",
      "         6.5976e-01,  6.7198e-01,  4.8715e-01,  6.6706e-01, -2.1024e-01,\n",
      "         5.2176e-01,  1.5666e+00,  1.4264e+00,  1.3276e+00,  9.3065e-01,\n",
      "         7.4780e-01,  5.0302e-01,  8.3869e-01,  4.6252e-01,  6.0331e-01,\n",
      "        -4.3424e-01,  3.3804e-01, -9.1551e-02,  9.5697e-04, -1.1556e-01,\n",
      "         9.6502e-01,  1.0015e+00,  7.5716e-01, -5.6124e-01, -1.7732e-02,\n",
      "         3.8734e-01,  7.1036e-02,  7.1527e-01,  4.3814e-01,  1.9849e-01,\n",
      "         1.6767e+00,  8.0482e-01,  3.5984e-01,  2.4527e-01,  1.2280e-02,\n",
      "        -6.7375e-01, -6.0129e-01,  2.8772e-01, -4.7205e-01,  8.5371e-01,\n",
      "        -1.0510e+00,  2.2924e-01,  4.7971e-01, -3.4659e-01,  7.9653e-01,\n",
      "         9.2289e-01,  2.6653e-01,  4.7438e-01,  2.8137e-01, -4.5832e-01,\n",
      "        -3.9109e-01, -1.0843e-01,  4.8959e-01,  2.5326e-01,  2.2360e-01,\n",
      "        -1.5306e-01, -9.4336e-01,  1.4466e-01, -9.7839e-02,  1.0856e+00,\n",
      "        -7.8785e-02, -6.9883e-01,  1.0801e-01, -4.2980e-01,  1.4559e-01,\n",
      "        -4.1568e-01, -1.8292e-01,  6.0722e-01, -5.1895e-01,  8.1169e-02,\n",
      "         6.8054e-01,  1.0313e-01, -2.9365e-01, -8.6038e-02, -9.2515e-02,\n",
      "        -7.3519e-02,  1.2243e+00,  2.6169e-01, -1.9785e-01,  9.2410e-01,\n",
      "         4.3144e-01, -1.4784e-01, -7.4512e-02,  1.9510e-01, -4.3457e-01,\n",
      "        -3.7168e-01,  4.3645e-02, -4.2856e-02, -4.9467e-02,  5.4636e-01,\n",
      "         4.2405e-02, -1.6125e-02,  4.8540e-01, -6.1863e-02, -2.8044e-01,\n",
      "        -4.6313e-01, -6.4515e-01, -5.2830e-01, -5.9209e-01, -5.6111e-01,\n",
      "         6.5110e-01,  4.0282e-01, -5.7228e-01, -7.3573e-01, -1.1195e+00,\n",
      "         5.3919e-01,  3.5429e-01, -7.9806e-02, -6.5355e-01,  9.1799e-02,\n",
      "        -3.2044e-01,  3.7976e-01, -3.0776e-01,  2.4895e-01,  1.0790e+00,\n",
      "         5.4836e-01, -2.7034e-01, -3.1488e-01, -7.9882e-01, -1.3152e+00,\n",
      "        -5.3945e-01, -4.5556e-01, -1.8871e-01, -8.2019e-01, -4.7875e-01,\n",
      "        -2.9585e-01,  4.8333e-01, -3.8106e-02, -9.6433e-01,  1.0548e-01,\n",
      "        -2.7646e-02,  6.4835e-01,  1.0219e-01, -3.7273e-01,  2.2994e-01,\n",
      "         2.1465e-01,  6.5202e-01, -2.0917e-01,  1.1036e+00, -1.1765e-01,\n",
      "         3.7134e-01, -5.5735e-01, -8.3370e-01, -1.9192e-01,  4.4559e-01,\n",
      "        -5.9554e-01, -2.6822e-01, -1.7543e-01,  3.8390e-02, -5.4983e-01,\n",
      "         9.2252e-02, -7.9436e-01, -2.9619e-01, -3.5526e-01, -4.0452e-02,\n",
      "         2.3259e-01,  2.3836e-01, -1.9541e-01,  4.2582e-01, -1.3100e-01,\n",
      "         3.8483e-01, -7.6399e-01, -9.8352e-01, -2.7336e-01, -4.1180e-01,\n",
      "        -1.9749e-01, -5.6832e-01,  2.6498e-01,  3.1050e-01,  7.8939e-02,\n",
      "        -2.3246e-01, -9.1441e-01,  9.0000e-01, -1.0770e-01,  1.3827e-01,\n",
      "        -3.5304e-01, -1.0673e-01,  1.2704e-01,  3.2683e-01, -6.9084e-01,\n",
      "         8.0355e-01,  5.0804e-01, -6.3075e-01,  3.6196e-01, -1.2306e-01,\n",
      "         1.0694e-01,  3.8078e-01,  9.9993e-02,  5.3174e-01,  2.7986e-01,\n",
      "        -2.9843e-02, -1.1371e-01,  3.9859e-02, -1.6170e-01, -1.2259e+00,\n",
      "        -1.6633e-02, -2.5087e-01, -7.8104e-02,  2.0518e-01, -3.1792e-01,\n",
      "        -3.2948e-01, -3.4098e-01, -6.5896e-02,  1.7969e-01, -5.3803e-01,\n",
      "        -6.1966e-01, -3.2685e-02, -3.8294e-01,  5.2765e-01,  4.6534e-02,\n",
      "         5.0007e-01,  9.3545e-02, -7.2668e-01, -4.8001e-01, -5.8956e-01,\n",
      "         3.5774e-01, -8.0450e-01, -9.1543e-01, -6.7784e-01, -4.8206e-01,\n",
      "        -2.9922e-01, -5.8634e-01,  8.9438e-02,  3.2149e-01,  7.4970e-01,\n",
      "        -4.2862e-01,  3.6758e-01, -3.8577e-01, -4.8010e-01, -8.6179e-01,\n",
      "         6.5745e-02, -5.1943e-01,  3.2644e-01,  3.4095e-01,  2.1394e-01,\n",
      "        -1.9000e-01, -4.0425e-01, -3.9842e-02, -8.5933e-01, -7.5438e-01,\n",
      "        -1.3140e-01, -8.1486e-01, -6.9522e-01, -5.4241e-01, -1.1368e-01,\n",
      "         4.5586e-01, -3.9155e-01,  8.9690e-02, -4.1146e-01,  5.7538e-01,\n",
      "         6.8459e-01, -1.5360e-01, -1.0039e-02,  2.6630e-01,  5.3178e-01,\n",
      "        -1.0098e-01, -3.9454e-01, -7.6259e-02, -4.5342e-01, -2.1171e-01,\n",
      "        -8.1541e-01,  3.3231e-01,  2.8864e-02,  4.1386e-01,  2.4129e-01,\n",
      "         1.1683e+00,  6.2908e-01, -6.6135e-01, -6.7399e-01, -4.0867e-01,\n",
      "        -3.7968e-02, -4.7581e-01,  5.8498e-01, -9.5180e-01, -5.8706e-01,\n",
      "        -1.4626e-01,  2.0935e-01, -1.8581e-01,  1.6092e-01, -4.9047e-01,\n",
      "        -1.0227e+00, -3.9939e-01, -1.2448e+00, -3.2606e-01, -2.9241e-01,\n",
      "         4.0904e-01,  2.3763e-01, -4.6714e-01,  1.7172e-01, -7.2476e-01,\n",
      "         7.2462e-01,  2.3717e-01, -2.8714e-01,  7.6770e-01, -7.0767e-03,\n",
      "         1.9167e-01,  1.4083e-01, -1.7810e-01, -3.6391e-01,  3.4074e-01,\n",
      "        -3.3144e-01, -5.3289e-01, -3.0762e-01, -3.1885e-01, -1.2208e+00,\n",
      "        -1.0995e+00, -9.0915e-01, -6.8567e-01,  2.5914e-01, -7.3338e-01,\n",
      "         4.3024e-01, -8.5031e-01,  2.9273e-01, -4.3882e-01, -4.4469e-02,\n",
      "        -6.9961e-01, -6.4193e-02, -7.0398e-01,  5.7960e-02, -2.0918e-01,\n",
      "        -2.0012e-01,  7.0405e-03, -6.1174e-01,  3.1626e-01,  9.2218e-01,\n",
      "        -1.8243e-01,  2.4718e-01,  6.4103e-01,  9.4422e-01, -1.5750e-01,\n",
      "         2.9729e-01, -5.1785e-01, -4.2424e-03,  4.8428e-01, -6.0771e-02,\n",
      "         1.1293e+00,  1.1891e-01,  4.1631e-02, -2.4831e-01, -5.3661e-01,\n",
      "         2.6181e-01,  2.9716e-01,  2.6218e-01, -1.0603e-01,  1.4114e-01,\n",
      "        -3.2312e-01, -5.5729e-01,  2.3547e-01,  1.6713e-01,  4.7367e-02,\n",
      "        -5.5076e-01,  8.5284e-01, -1.0181e-01, -2.0538e-01, -2.4555e-01,\n",
      "         8.2134e-01, -8.6755e-01,  2.7286e-02, -4.4187e-01, -3.8785e-02,\n",
      "        -5.0579e-01, -1.1031e+00,  2.5061e-01, -3.1038e-02,  1.1086e+00,\n",
      "         3.3124e-01, -6.0941e-01, -8.9548e-01,  2.1434e-01,  5.3688e-03,\n",
      "        -1.7921e-01, -6.4796e-01,  6.8074e-01,  9.7088e-01, -4.1550e-01,\n",
      "         6.9095e-02, -1.2669e-01, -1.4970e-01,  5.8426e-04,  2.1021e-01,\n",
      "        -7.7738e-02,  2.6662e-01, -1.6539e-01,  7.9108e-01,  1.6196e-01,\n",
      "        -6.5884e-02, -7.5711e-02, -7.7196e-01, -5.4624e-02, -1.5122e+00,\n",
      "        -2.7658e-01, -8.0171e-01, -3.5934e-01, -8.1233e-02, -1.8494e-01,\n",
      "        -1.1469e+00,  8.4736e-01, -9.0092e-02,  3.1603e-02,  4.6041e-01,\n",
      "        -1.0109e+00,  3.9672e-02,  7.9934e-01, -7.1626e-01,  3.8910e-01,\n",
      "         4.2321e-01, -4.1682e-01, -2.5857e-01, -9.6427e-02,  1.8137e-02,\n",
      "        -1.2119e+00, -8.9199e-01, -8.3362e-01, -1.8739e-01, -7.9333e-01,\n",
      "        -7.3028e-01, -2.3050e-01,  5.7775e-01,  3.0574e-01,  1.1964e+00,\n",
      "        -2.5237e-02, -7.1485e-01,  1.4907e-01, -6.0832e-01, -2.4601e-01,\n",
      "        -6.3292e-01, -1.2756e-01, -2.0221e-01, -6.6342e-01, -3.8806e-01,\n",
      "        -7.6341e-02,  7.6254e-01,  6.8854e-01,  9.0490e-01, -3.3344e-01,\n",
      "        -6.1917e-01, -1.2944e-01,  7.5298e-03, -3.1824e-01,  3.6401e-01,\n",
      "        -7.4528e-01, -4.3407e-01, -8.7013e-01,  8.2502e-01, -6.7501e-01,\n",
      "        -6.2392e-01,  4.5225e-03, -6.2716e-01,  9.1860e-01,  3.3473e-01,\n",
      "         2.2756e-01, -1.4161e-01, -4.8308e-01, -4.9400e-01, -1.3904e-01,\n",
      "         5.2262e-01, -6.2047e-02,  4.5611e-02,  7.5659e-02, -7.4595e-01,\n",
      "         1.3035e+00, -7.5399e-01,  1.9588e-01, -4.7686e-01, -5.4722e-01,\n",
      "        -3.0659e-01, -4.0915e-01, -3.1164e-01, -5.8960e-01, -4.3287e-01,\n",
      "        -1.7568e-01,  2.3586e-01,  6.5345e-01, -3.4270e-01, -1.0826e+00,\n",
      "        -5.1963e-01,  1.2301e+00, -6.5531e-01, -1.8719e-01, -6.6242e-01,\n",
      "         1.9552e-02, -9.3524e-01, -8.9159e-02,  8.8725e-02, -6.7072e-01,\n",
      "         2.2470e-01, -1.2183e+00, -4.7442e-01, -1.4336e+00, -5.7047e-01,\n",
      "        -8.1253e-02, -3.1530e-01, -5.4892e-01, -3.7068e-02,  7.6798e-02,\n",
      "        -4.2875e-01, -2.2915e-01,  1.6278e-01,  6.8609e-01, -2.1586e-01,\n",
      "        -1.7329e+00,  1.9508e-01,  9.0273e-01,  2.4986e-01, -1.1510e+00,\n",
      "         3.3117e-01,  4.0132e-01, -3.2921e-01,  3.1862e-01,  6.6657e-02,\n",
      "        -7.4699e-02, -1.2102e-01, -4.9814e-01,  6.0057e-01, -1.0304e+00,\n",
      "        -6.7029e-01,  2.9358e-01, -1.5397e-01, -4.7470e-01, -1.0401e+00,\n",
      "        -2.2276e-01, -2.0235e-01, -4.3432e-02, -1.7475e-01,  2.9698e-01,\n",
      "        -3.3394e-01, -6.2594e-02,  4.3515e-02,  3.9609e-01, -1.7743e-01,\n",
      "        -3.5637e-02, -4.4612e-01, -6.6403e-01, -4.9458e-01,  1.1151e-01,\n",
      "        -1.6449e+00, -2.0385e-01, -3.6529e-01,  2.3514e-01,  9.2044e-02,\n",
      "         4.9115e-01, -3.1712e-01,  1.0172e+00, -1.1309e-01, -9.3689e-01,\n",
      "         6.5912e-02, -5.0505e-01, -6.9275e-01, -5.1713e-01, -5.4040e-01,\n",
      "         1.9027e-02,  3.0596e-01,  8.6705e-02, -8.7342e-01,  6.4824e-01,\n",
      "         1.3304e-01, -1.9116e-01, -3.1483e-01, -2.1642e-01, -1.1408e+00,\n",
      "        -1.3010e-01,  1.5594e-01,  1.0394e+00,  4.8087e-01, -4.7832e-01,\n",
      "        -3.7086e-01,  6.9584e-01, -7.7515e-01,  3.5141e-01,  2.4902e-01,\n",
      "        -6.5666e-01, -1.1391e+00, -6.2508e-01,  2.5133e-01,  1.8124e-01,\n",
      "        -4.3562e-01, -9.2221e-01,  2.4111e-01, -8.6781e-02,  2.2504e-01,\n",
      "        -5.9971e-01, -3.7258e-01,  3.9819e-01,  9.4447e-02, -5.1777e-01,\n",
      "         2.7408e-01, -6.9928e-02,  1.8522e-02, -2.7248e-01, -3.5756e-01,\n",
      "         2.6874e-01, -2.8751e-01, -1.1791e+00, -1.0137e-01,  5.8522e-01,\n",
      "        -1.3387e-01,  5.5192e-01,  1.2090e-01, -4.8306e-01,  2.8695e-01,\n",
      "         3.1226e-01,  4.0994e-02,  9.9969e-01,  8.8203e-02, -6.5463e-02,\n",
      "         8.6407e-02,  1.8305e-01,  1.9750e-01,  5.1459e-01,  4.9369e-04,\n",
      "        -6.8712e-02, -3.9732e-01, -4.2121e-02, -5.0291e-02, -5.7289e-01,\n",
      "        -3.3844e-01, -2.0321e-01,  2.8725e-01,  5.4256e-02, -2.3246e-02,\n",
      "         5.7341e-02,  4.5290e-01, -7.2808e-02, -7.9143e-02,  3.6966e-01,\n",
      "        -5.8449e-02, -8.8013e-01,  5.9286e-01, -1.2901e-01, -4.3557e-01,\n",
      "        -2.6178e-01,  5.2836e-01,  5.5729e-03,  2.9477e-01,  5.8899e-01,\n",
      "         2.9884e-01,  2.6648e-01,  5.2306e-02, -1.3141e-01, -3.4227e-01,\n",
      "         1.2135e-01,  9.4543e-01,  1.5368e-01, -7.2386e-02,  7.4703e-01,\n",
      "         1.2070e+00,  7.6452e-01, -4.9940e-01,  3.9313e-01,  3.0450e-01])\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## START BUILDING CLASSIFICATION MODEL ON TOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights.\n"
     ]
    }
   ],
   "source": [
    "model_ft = ViT(model_name, pretrained=True)\n",
    "set_parameter_requires_grad(model_ft, feature_extracting=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = torch.nn.Linear(num_ftrs, train_dataset.num_labels)\n",
    "input_size = 384\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "print(\"Params to learn:\")\n",
    "params_to_update = []\n",
    "for name,param in model_ft.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\",name)\n",
    "\n",
    "\n",
    "# Try the original transformer's optimizer\n",
    "optimizer_ft = torch.optim.AdamW(params_to_update, lr=1e-9, betas=(.9, .98))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import copy\n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 1.2343 Acc: 0.5564\n",
      "val Loss: 2.0127 Acc: 0.4306\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 1.3610 Acc: 0.5616\n",
      "val Loss: 2.0127 Acc: 0.4306\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 1.2989 Acc: 0.5642\n",
      "val Loss: 2.0127 Acc: 0.4306\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 1.3077 Acc: 0.5469\n",
      "val Loss: 2.0127 Acc: 0.4306\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 1.2592 Acc: 0.5712\n",
      "val Loss: 2.0127 Acc: 0.4306\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 1.2641 Acc: 0.5668\n",
      "val Loss: 2.0127 Acc: 0.4306\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 1.2466 Acc: 0.5642\n",
      "val Loss: 2.0127 Acc: 0.4306\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 1.3498 Acc: 0.5582\n",
      "val Loss: 2.0127 Acc: 0.4306\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 1.2547 Acc: 0.5729\n",
      "val Loss: 2.0127 Acc: 0.4306\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 1.3134 Acc: 0.5443\n",
      "val Loss: 2.0127 Acc: 0.4306\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 1.3422 Acc: 0.5486\n",
      "val Loss: 2.0127 Acc: 0.4306\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 1.3162 Acc: 0.5469\n",
      "val Loss: 2.0127 Acc: 0.4306\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 1.3175 Acc: 0.5556\n",
      "val Loss: 2.0127 Acc: 0.4306\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 1.2558 Acc: 0.5720\n",
      "val Loss: 2.0127 Acc: 0.4306\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 1.2790 Acc: 0.5677\n",
      "val Loss: 2.0127 Acc: 0.4306\n",
      "\n",
      "Training complete in 31m 39s\n",
      "Best val Acc: 0.430556\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "train_dataset = TransformerDataset(train)\n",
    "val_dataset = TransformerDataset(val)\n",
    "test_dataset = TransformerDataset(test)\n",
    "dataloaders_dict = {'train': torch.utils.data.DataLoader(train_dataset), \n",
    "                    'val': torch.utils.data.DataLoader(val_dataset), \n",
    "                    'test': torch.utils.data.DataLoader(test_dataset)}\n",
    "# Train and evaluate\n",
    "num_epochs = 15\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft, \"visual_transformer_mel_cropped_stacked_freeze_layers_lr1e-9_Adam.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ohist = [h.cpu().numpy() for h in hist]\n",
    "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
