{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import transformers\n",
    "import torchvision\n",
    "from pytorch_pretrained_vit import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'B_16_imagenet1k'\n",
    "model = ViT(model_name, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/preprocessed_data_split_nona_03_07.pkl', 'rb') as f:\n",
    "    train, val, test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>intensity</th>\n",
       "      <th>statement</th>\n",
       "      <th>repeat</th>\n",
       "      <th>gender</th>\n",
       "      <th>mel</th>\n",
       "      <th>mfcc</th>\n",
       "      <th>chromagram</th>\n",
       "      <th>spec_contrast</th>\n",
       "      <th>tonnetz</th>\n",
       "      <th>filename</th>\n",
       "      <th>mel_pad</th>\n",
       "      <th>mfcc_pad</th>\n",
       "      <th>chromagram_pad</th>\n",
       "      <th>spec_contrast_pad</th>\n",
       "      <th>tonnetz_pad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[[9.50724e-05, 6.8767185e-05, 0.00015295653, 0...</td>\n",
       "      <td>[[-884.38464, -885.6134, -884.1467, -882.37726...</td>\n",
       "      <td>[[0.7087542, 0.47900087, 0.6418329, 0.93395066...</td>\n",
       "      <td>[[24.56065813501899, 16.53363965318033, 21.898...</td>\n",
       "      <td>[[-0.07230143814529716, -0.07546215010665616, ...</td>\n",
       "      <td>03-01-02-01-02-02-08.wav</td>\n",
       "      <td>[[9.507239883532748e-05, 6.876718543935567e-05...</td>\n",
       "      <td>[[-884.3846435546875, -885.6134033203125, -884...</td>\n",
       "      <td>[[0.7087541818618774, 0.47900086641311646, 0.6...</td>\n",
       "      <td>[[24.56065813501899, 16.53363965318033, 21.898...</td>\n",
       "      <td>[[-0.07230143814529716, -0.07546215010665616, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[[1.789445e-06, 1.2919778e-06, 1.0217702e-06, ...</td>\n",
       "      <td>[[-666.02386, -656.1765, -645.44977, -641.7728...</td>\n",
       "      <td>[[0.9456255, 0.52619433, 0.62148577, 0.7980577...</td>\n",
       "      <td>[[16.717965213015603, 18.446617295720543, 40.8...</td>\n",
       "      <td>[[-0.022028304875560848, -0.0190684326124428, ...</td>\n",
       "      <td>03-01-06-01-01-01-10.wav</td>\n",
       "      <td>[[1.7894450365929515e-06, 1.2919778100695112e-...</td>\n",
       "      <td>[[-666.0238647460938, -656.176513671875, -645....</td>\n",
       "      <td>[[0.9456254839897156, 0.5261943340301514, 0.62...</td>\n",
       "      <td>[[16.717965213015603, 18.446617295720543, 40.8...</td>\n",
       "      <td>[[-0.022028304875560848, -0.0190684326124428, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[[8.2225015e-13, 2.3429403e-11, 2.0612288e-11,...</td>\n",
       "      <td>[[-778.2969, -778.2969, -778.2969, -778.2969, ...</td>\n",
       "      <td>[[0.94348556, 0.9370095, 0.94074184, 0.9504772...</td>\n",
       "      <td>[[8.039895497077225, 5.939298528871646, 3.8563...</td>\n",
       "      <td>[[-0.16673182857144367, -0.18327695918207515, ...</td>\n",
       "      <td>03-01-07-01-02-01-16.wav</td>\n",
       "      <td>[[8.222501455754094e-13, 2.342940251531811e-11...</td>\n",
       "      <td>[[-778.296875, -778.296875, -778.296875, -778....</td>\n",
       "      <td>[[0.9434855580329895, 0.9370095133781433, 0.94...</td>\n",
       "      <td>[[8.039895497077225, 5.939298528871646, 3.8563...</td>\n",
       "      <td>[[-0.16673182857144367, -0.18327695918207515, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[-668.9862, -668.9862, -668.9862, -668.9862, ...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[20.608299523328014, 20.608299523328014, 20.6...</td>\n",
       "      <td>[[0.1238472328395454, 0.10053637477785138, 0.0...</td>\n",
       "      <td>03-01-05-01-02-01-24.wav</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[-668.9862060546875, -668.9862060546875, -668...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[20.608299523328014, 20.608299523328014, 20.6...</td>\n",
       "      <td>[[0.1238472328395454, 0.10053637477785138, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[[3.2012485e-05, 1.0481453e-05, 4.1928706e-06,...</td>\n",
       "      <td>[[-755.72424, -756.8351, -756.4492, -757.50037...</td>\n",
       "      <td>[[0.2619576, 0.43366244, 0.61586565, 0.6568907...</td>\n",
       "      <td>[[15.546648988556463, 15.769111896492403, 17.7...</td>\n",
       "      <td>[[-0.042034618409396124, -0.037828358447922365...</td>\n",
       "      <td>03-01-04-02-02-01-10.wav</td>\n",
       "      <td>[[3.2012485462473705e-05, 1.0481452591193374e-...</td>\n",
       "      <td>[[-755.7242431640625, -756.8350830078125, -756...</td>\n",
       "      <td>[[0.261957585811615, 0.43366244435310364, 0.61...</td>\n",
       "      <td>[[15.546648988556463, 15.769111896492403, 17.7...</td>\n",
       "      <td>[[-0.042034618409396124, -0.037828358447922365...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      emotion  intensity  statement  repeat  gender  \\\n",
       "369         2          1          2       2       0   \n",
       "1255        6          1          1       1       0   \n",
       "592         7          1          2       1       0   \n",
       "1290        5          1          2       1       0   \n",
       "1240        4          2          2       1       0   \n",
       "\n",
       "                                                    mel  \\\n",
       "369   [[9.50724e-05, 6.8767185e-05, 0.00015295653, 0...   \n",
       "1255  [[1.789445e-06, 1.2919778e-06, 1.0217702e-06, ...   \n",
       "592   [[8.2225015e-13, 2.3429403e-11, 2.0612288e-11,...   \n",
       "1290  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1240  [[3.2012485e-05, 1.0481453e-05, 4.1928706e-06,...   \n",
       "\n",
       "                                                   mfcc  \\\n",
       "369   [[-884.38464, -885.6134, -884.1467, -882.37726...   \n",
       "1255  [[-666.02386, -656.1765, -645.44977, -641.7728...   \n",
       "592   [[-778.2969, -778.2969, -778.2969, -778.2969, ...   \n",
       "1290  [[-668.9862, -668.9862, -668.9862, -668.9862, ...   \n",
       "1240  [[-755.72424, -756.8351, -756.4492, -757.50037...   \n",
       "\n",
       "                                             chromagram  \\\n",
       "369   [[0.7087542, 0.47900087, 0.6418329, 0.93395066...   \n",
       "1255  [[0.9456255, 0.52619433, 0.62148577, 0.7980577...   \n",
       "592   [[0.94348556, 0.9370095, 0.94074184, 0.9504772...   \n",
       "1290  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1240  [[0.2619576, 0.43366244, 0.61586565, 0.6568907...   \n",
       "\n",
       "                                          spec_contrast  \\\n",
       "369   [[24.56065813501899, 16.53363965318033, 21.898...   \n",
       "1255  [[16.717965213015603, 18.446617295720543, 40.8...   \n",
       "592   [[8.039895497077225, 5.939298528871646, 3.8563...   \n",
       "1290  [[20.608299523328014, 20.608299523328014, 20.6...   \n",
       "1240  [[15.546648988556463, 15.769111896492403, 17.7...   \n",
       "\n",
       "                                                tonnetz  \\\n",
       "369   [[-0.07230143814529716, -0.07546215010665616, ...   \n",
       "1255  [[-0.022028304875560848, -0.0190684326124428, ...   \n",
       "592   [[-0.16673182857144367, -0.18327695918207515, ...   \n",
       "1290  [[0.1238472328395454, 0.10053637477785138, 0.0...   \n",
       "1240  [[-0.042034618409396124, -0.037828358447922365...   \n",
       "\n",
       "                      filename  \\\n",
       "369   03-01-02-01-02-02-08.wav   \n",
       "1255  03-01-06-01-01-01-10.wav   \n",
       "592   03-01-07-01-02-01-16.wav   \n",
       "1290  03-01-05-01-02-01-24.wav   \n",
       "1240  03-01-04-02-02-01-10.wav   \n",
       "\n",
       "                                                mel_pad  \\\n",
       "369   [[9.507239883532748e-05, 6.876718543935567e-05...   \n",
       "1255  [[1.7894450365929515e-06, 1.2919778100695112e-...   \n",
       "592   [[8.222501455754094e-13, 2.342940251531811e-11...   \n",
       "1290  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1240  [[3.2012485462473705e-05, 1.0481452591193374e-...   \n",
       "\n",
       "                                               mfcc_pad  \\\n",
       "369   [[-884.3846435546875, -885.6134033203125, -884...   \n",
       "1255  [[-666.0238647460938, -656.176513671875, -645....   \n",
       "592   [[-778.296875, -778.296875, -778.296875, -778....   \n",
       "1290  [[-668.9862060546875, -668.9862060546875, -668...   \n",
       "1240  [[-755.7242431640625, -756.8350830078125, -756...   \n",
       "\n",
       "                                         chromagram_pad  \\\n",
       "369   [[0.7087541818618774, 0.47900086641311646, 0.6...   \n",
       "1255  [[0.9456254839897156, 0.5261943340301514, 0.62...   \n",
       "592   [[0.9434855580329895, 0.9370095133781433, 0.94...   \n",
       "1290  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1240  [[0.261957585811615, 0.43366244435310364, 0.61...   \n",
       "\n",
       "                                      spec_contrast_pad  \\\n",
       "369   [[24.56065813501899, 16.53363965318033, 21.898...   \n",
       "1255  [[16.717965213015603, 18.446617295720543, 40.8...   \n",
       "592   [[8.039895497077225, 5.939298528871646, 3.8563...   \n",
       "1290  [[20.608299523328014, 20.608299523328014, 20.6...   \n",
       "1240  [[15.546648988556463, 15.769111896492403, 17.7...   \n",
       "\n",
       "                                            tonnetz_pad  \n",
       "369   [[-0.07230143814529716, -0.07546215010665616, ...  \n",
       "1255  [[-0.022028304875560848, -0.0190684326124428, ...  \n",
       "592   [[-0.16673182857144367, -0.18327695918207515, ...  \n",
       "1290  [[0.1238472328395454, 0.10053637477785138, 0.0...  \n",
       "1240  [[-0.042034618409396124, -0.037828358447922365...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 250)\n",
      "(6, 250)\n",
      "(20, 250)\n",
      "(12, 250)\n",
      "(7, 250)\n",
      "(384, 384)\n",
      "1152\n"
     ]
    }
   ],
   "source": [
    "# 256 + 6 + 20 + 12 + 7 = 256 + 45 = 301\n",
    "print(train['mel_pad'][0].shape)\n",
    "print(train['tonnetz_pad'][0].shape)\n",
    "print(train['mfcc_pad'][0].shape)\n",
    "print(train['chromagram_pad'][0].shape)\n",
    "print(train['spec_contrast_pad'][0].shape)\n",
    "print(model.image_size)\n",
    "print(len(train['mel_pad']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# implementing a mel-only version\n",
    "class TransformerDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.labels = df['emotion'].reset_index(drop=True)\n",
    "        self.num_labels = self.labels.nunique()\n",
    "        self.mels = df['mel_pad'].reset_index(drop=True)\n",
    "        self.max_len = self.mels[0].shape[1]\n",
    "        self.transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                             transforms.CenterCrop((self.max_len, self.max_len)),\n",
    "                                             transforms.Resize(model.image_size)])\n",
    "                                             #transforms.Normalize([0.5, 0.5, 0.5],\n",
    "                                                                  #[0.5, 0.5, 0.5]),])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.mels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        mel = self.mels.iloc[idx]\n",
    "        label = self.labels.iloc[idx]        \n",
    "        mel = self.transform(mel)\n",
    "        # stack the same mel spectrogram three times to emulate RGB image\n",
    "        mel = torch.stack([mel]*3, dim=1).squeeze(dim=0).type(torch.float)\n",
    "        label = torch.tensor(label-1).type(torch.long)\n",
    "        \n",
    "        return mel, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with a \"TopCrop\" instead of a CenterCrop\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "# implementing a mel-only version\n",
    "class TransformerTopCropDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.labels = df['emotion'].reset_index(drop=True)\n",
    "        self.num_labels = self.labels.nunique()\n",
    "        self.mels = df['mel_pad'].reset_index(drop=True)\n",
    "        self.max_len = self.mels[0].shape[1]\n",
    "        self.transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                             transforms.Resize(model.image_size)])\n",
    "                                             #transforms.Normalize([0.5, 0.5, 0.5],\n",
    "                                                                  #[0.5, 0.5, 0.5]),])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.mels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        mel = self.mels.iloc[idx]\n",
    "        label = self.labels.iloc[idx]        \n",
    "        mel = self.transform(mel[:250,])\n",
    "        # stack the same mel spectrogram three times to emulate RGB image\n",
    "        mel = torch.stack([mel]*3, dim=1).squeeze(dim=0).type(torch.float)\n",
    "        label = torch.tensor(label-1).type(torch.long)\n",
    "        \n",
    "        return mel, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1152\n",
      "torch.Size([3, 384, 384])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TransformerDataset(train)\n",
    "print(len(train_dataset))\n",
    "print(train_dataset[0][0].shape)\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "img = train_dataset[0][0]\n",
    "with torch.no_grad():\n",
    "    outputs = model(img.float().unsqueeze(0)).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block(\n",
      "  (attn): MultiHeadedSelfAttention(\n",
      "    (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pwff): PositionWiseFeedForward(\n",
      "    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.transformer.blocks[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## START BUILDING CLASSIFICATION MODEL ON TOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting, blocks=[]):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for block in blocks:\n",
    "            for param in model.transformer.blocks[block].parameters():\n",
    "                param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import copy\n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights.\n"
     ]
    }
   ],
   "source": [
    "model_ft = ViT(model_name, pretrained=True)\n",
    "set_parameter_requires_grad(model_ft, feature_extracting=True, blocks=[11])\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = torch.nn.Linear(num_ftrs, train_dataset.num_labels)\n",
    "input_size = 384\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t transformer.blocks.11.attn.proj_q.weight\n",
      "\t transformer.blocks.11.attn.proj_q.bias\n",
      "\t transformer.blocks.11.attn.proj_k.weight\n",
      "\t transformer.blocks.11.attn.proj_k.bias\n",
      "\t transformer.blocks.11.attn.proj_v.weight\n",
      "\t transformer.blocks.11.attn.proj_v.bias\n",
      "\t transformer.blocks.11.proj.weight\n",
      "\t transformer.blocks.11.proj.bias\n",
      "\t transformer.blocks.11.norm1.weight\n",
      "\t transformer.blocks.11.norm1.bias\n",
      "\t transformer.blocks.11.pwff.fc1.weight\n",
      "\t transformer.blocks.11.pwff.fc1.bias\n",
      "\t transformer.blocks.11.pwff.fc2.weight\n",
      "\t transformer.blocks.11.pwff.fc2.bias\n",
      "\t transformer.blocks.11.norm2.weight\n",
      "\t transformer.blocks.11.norm2.bias\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "\n",
    "# TODO: try unfreezing the last few layers/blocks as well\n",
    "print(\"Params to learn:\")\n",
    "params_to_update = []\n",
    "for name,param in model_ft.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\",name)\n",
    "\n",
    "\n",
    "# Try the original transformer's optimizer\n",
    "optimizer_ft = transformers.AdamW(params_to_update, lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 1.7915 Acc: 0.3108\n",
      "val Loss: 1.5955 Acc: 0.3889\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 1.5405 Acc: 0.4080\n",
      "val Loss: 1.5163 Acc: 0.4375\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 1.4226 Acc: 0.4549\n",
      "val Loss: 1.4506 Acc: 0.4722\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 1.2884 Acc: 0.5165\n",
      "val Loss: 1.4167 Acc: 0.4931\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 1.1994 Acc: 0.5347\n",
      "val Loss: 1.3990 Acc: 0.4792\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 1.1030 Acc: 0.5868\n",
      "val Loss: 1.4215 Acc: 0.4792\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 1.0529 Acc: 0.6241\n",
      "val Loss: 1.5352 Acc: 0.4306\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 0.9410 Acc: 0.6589\n",
      "val Loss: 1.5284 Acc: 0.4931\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 0.9243 Acc: 0.6788\n",
      "val Loss: 1.7434 Acc: 0.4722\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 0.8640 Acc: 0.6823\n",
      "val Loss: 1.5190 Acc: 0.4722\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 0.8196 Acc: 0.7023\n",
      "val Loss: 1.5331 Acc: 0.4653\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 0.7841 Acc: 0.7066\n",
      "val Loss: 1.5160 Acc: 0.4861\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 0.7571 Acc: 0.7161\n",
      "val Loss: 1.5930 Acc: 0.4931\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 0.7115 Acc: 0.7378\n",
      "val Loss: 1.6340 Acc: 0.5000\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 0.6592 Acc: 0.7552\n",
      "val Loss: 1.5780 Acc: 0.4931\n",
      "\n",
      "Training complete in 36m 16s\n",
      "Best val Acc: 0.500000\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "train_dataset = TransformerDataset(train)\n",
    "val_dataset = TransformerDataset(val)\n",
    "test_dataset = TransformerDataset(test)\n",
    "dataloaders_dict = {'train': torch.utils.data.DataLoader(train_dataset), \n",
    "                    'val': torch.utils.data.DataLoader(val_dataset), \n",
    "                    'test': torch.utils.data.DataLoader(test_dataset)}\n",
    "# Train and evaluate\n",
    "num_epochs = 15\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft, \"visual_transformer_mel_cropped_stacked_freeze_toblock11_lr5e-5_AdamW.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAwxUlEQVR4nO3deZxWdd3/8debYYadGUBQYEBwQxGBFFEzxX3LJS1bNEvTzLu05Xff3Vl2l911t9x3ZWUmkalZWt5aKnmr2SKumEICgpoiiwyILMqwwyyf3x/nDFxczHIxc10zMNf7+XjMY876Od/rXNc5n3O+53vOUURgZmbFq0tHF8DMzDqWE4GZWZFzIjAzK3JOBGZmRc6JwMysyDkRmJkVOSeCXSApJB2Qdk+W9B+5TNuK5Vws6dHWltM6B0knSKrqwOWfL2mJpPWS3lXA5cyTdEK+p93dSbpe0m86uhxQZIlA0p8k/Wcjw8+TtFxS11xjRcRVEfHNPJRpRJo0ti07Iu6MiNPaGruZZY6UVC/pZ4VaRmeUbrgh6cKMYV3TYSM6sGiF8n3g6ojoHREvNAyUNDxNDg1/IWlDRv9xu7KQiDg0Iqble9pdIelSSXVZn2u9pCH5XtbuqKgSAXA7cIkkZQ2/BLgzImrbv0gd4mPAO8CHJXVrzwVLKmnP5RXA28B/7mmfY1cOcjLsC8zLHhgRb6TJoXdE9E4Hj8sY9mQbl9tRpmd+rvRvWUcXqj0UWyK4H+gPbDtikdQPOBu4Q9JESdMlrZH0pqSfSiprLJCk2yV9K6P/i+k8yyR9Imva90p6QdLa9FT7+ozRT6T/16RHIMekRydPZcz/bknPS6pO/787Y9w0Sd+U9LSkdZIelbRXC+vhY8BXgRrgnKyynidpVlrW1yWdkQ7vL+m29PO9I+n+dPgOZU2HZVah3S7pZkkPSdoAnNjC+kDSeyQ9k34PS9JlHCnprcwdi6T3S5qV/eEkHZ2e4ZVkDDtf0py0e6KkGeny35L0wxbWV6ZHgK3ARxsbmX4fV2T0Z3+XIenTkl5Lv69vSto//d2tlfS/2b85SV+RtErSIkkXZwzvJun7kt5IP8dkST3ScSdIqpL0JUnLgdsaKWsXSV+VtFjSCkl3SCpP464HSoDZkl7PdeWkn/dpSTdIehu4Pv18f5O0Ov0cd0qqyJhnkaRT0u7r03VwR7p+5kma0MppD09/Z+sk3SPpbmVss7siXe6XJb2U/v5vk9Q9Y/wnJc2X9Lakqco4k5B0qKQ/p+PekvSVjNBlzZT/S5KWpuP+Kenk1pQ9JxFRVH/AL4BbMvo/BcxKu48Ajga6AiOAl4HPZ0wbwAFp9+3At9LuM4C3gDFAL+CurGlPAA4jSbxj02nfl44bkU7bNWM5lwJPpd39SY7eL0nL9ZG0f0A6fhrwOnAQ0CPt/24zn/84YAvQD7gRmJoxbiJQDZyalnUocHA67v+Au9P5SoFJ2WVtZj1VA8emMbu3sD6GA+vSz1kKDADGp+NeAs7MWM59wL828TlfB07N6L8HuDbtng5cknb3Bo7O8bdzPfAb4FxgQVq+runnHZHxfVzR2HeZsW6mAn2BQ9Pv4q/AfkB5+hk/nvG7qQV+CHQDJgEbgFHp+B+lsfoDfYA/At/Jmvd76bw9Gvk8nwDmp8vuDfwB+HVj32ML6yXz+740Xe416brpARxA8pvqBgwkOfj5Ucb8i4BTMtbxZuAskkT0HeDZXZ0WKAMWA59Lv6cLSBL4t5r4DDt8T42MXwTMBYal6/tptm//JwGrgMPTz3gj8EQ6rg/wJvCvJL/9PsBROZR/FLAEGJKxn9i/YPvFQgXeXf+A95DsmHqk/U8DX2hi2s8D9zXxg78944dwKxk7X5KdcpMbEckGfEPGF9xcIrgEeC5r/unApWn3NOCrGeM+DTzSzOe/Bbg/7T6G5KxgUNr/84ZyZc0zGKgH+jUybqcNqJH1dEcL30nm+vhy5jrPmu5LJFV4pBvjRmBwE9N+C7g17e5DsgPdN+1/AvgGsNcu/nauB36Tdv8d+BdalwiOzeifCXwpo/8HpDtJtu/Me2WM/1/gPwCln2n/jHHHAAsz5t0KdG/m8/wV+HRG/6j099A1+3tsYb1kJ4I3Wpj+fcALGf2L2HHn/peMcaOBTbs6LXA8sBRQxvinaD4R1AJrMv5ez1ruVRn9ZzWMB34J/HfGuN7pehxBckDzQhPLbK78BwArgFOA0l35nbbmr9iqhoiIp4CVwHmS9gOOJDmCR9JBkh5MqxXWAt8GWqpmARhCkr0bLM4cKekoSY9JWimpGrgqx7gNsRdnDVtMcrTeYHlG90aSH+JO0mqDC4E7ASJiOvAGcFE6yTCSI+lsw4C3I+KdHMucLXPdtLQ+mioDJEfj50jqDXwQeDIi3mxi2ruAC5RcA7kA+EdENKzHy0mS9StKqtrObsVn+ipwHclR3q56K6N7UyP9md/fOxGxIaN/MclvYiDQE5ippAptDUm11cCMaVdGxOZmypH921pMktj2zvFzNCX7+x4k6XdpNcdaku+xud9/9u+5u5q+1tDUtEOApZHuVRsrVyOejYiKjL/9s8Znb+MN1T87rMeIWA+sJtlGm/s9N1n+iJhPciB6PbAiXX8Fu3BddIkgdQdJPfklwKMR0bAh3gy8AhwYEX2Br5AcebXkTZIvvMHwrPF3kZzCD4uIcmByRtygectILtplGk5ytLOrziepkvhZmuyWk/xYP5aOXwJk//gbhvfPrNfNsIFkhwSApH0amSb7Mza3PpoqAxGxlORs6HyS7+7XjU2XTvsSycZ5Jkmiuytj3GsR8RFgEEnVyb2SejUVq4n4fyapVvl01qgd1gfQ2PrYFf2yyjac5DexiiRpHJqx4yqP7RdvYdd/W8NJjorfanzynGUv9zvpsLHpdvVRctuu2uJNYKi0Q8OQYU1NnKPsbbzhQvIO6zH9vgaQbKNN/p5bEhF3RcR70thB8lstiGJOBKcAnwR+lTG8D7AWWC/pYJJT/1z8L3CppNGSegJfzxrfh+SIerOkiWw/Aofk7KSepJ62MQ8BB0m6SElTxQ+RnEI+mGPZMn2cpBrrMGB8+ncsMF7SYSSnuJdJOjm9kDhU0sHpUffDJAmkn6RSScenMWcDh0oan148uz6HcjS3Pu4ETpH0wfTzDpA0PmP8HcC/p5/hvhaWcxfwWZJqgnsaBkr6qKSBEVFPUgUAUJdDubNdl5Yl0yySM5GeSi6YX96KuNm+IalMSbPMs4F70rL/ArhB0iCA9Ps6fRfi/hb4gpLmxL1JzoDvjvy3nusDrCdpEDEU+GKe4zdmOsl3enX6OzqP5BpYW3xGUqWk/iQHiXenw+8i2W7Gp2eg3wb+HhGLSLbTfSR9XslF+D6SjmppQZJGSTopjbeZJOm35jeak6JMBOkX9AzJhd2pGaP+jWSntI5kI7t7p5kbj/cwST3330iOEv+WNcmnSZocrgO+RpI4GubdCPwX8HR6in90VuzVJBv/v5Kcbv47cHZErMqlbA3SDfBkkvrn5Rl/M0mqFD4eEc8BlwE3kFxHeZztRzqXkNR7vkJSd/n5tHyvAv8J/AV4jaQetiXNrY83SOpf/5WkqeYsYFzGvPelZbovq8qkMb8lqSv/W9b6OgOYp6RlzI+BDzdUoWgX2sFHxNPAc1mDbyCpm3+L5CDjzlxiNWM5SeOAZWmsqyLilXTcl0h+b8+mVS5/Iannz9WtJGdVTwALSXY417SxvI35BsmF1GqSRgd/KMAydhARW0mqBC8nSfYfJdkpb2lmtmO0830ER2aMvwt4lKShwAKS61BExF9Jrtv8nuRMZH/gw+m4dSQXys8h+S5fA07M4SN0A75Lcua3nOTs9SvNztEG2rEKzWz3p6Q546ci4i8dXRbbc0j6OzA5Im5rxbyLSBoBdMrfXFGeEdieS9L7SepLs8+6zHYgaZKkfdKqoY+TNFV+pKPLtTsqWCKQdKuSm1TmNjFekn6i5CaMOZIOL1RZrHOQNI3kgv5n0jpys+aMIrmGVU1S1fiBZlqZFbWCVQ2lFxPXk7QhH9PI+LNI6iPPAo4CfhwRLV5EMTOz/CrYGUFEPEFysa8p55EkiYiIZ4EKSYMLVR4zM2tcRz4Qaig73qBRlQ7b6dRN0pXAlQC9evU64uCDD26XApqZdRYzZ85cFREDGxvXkYmgsRtKGq2niogpwBSACRMmxIwZMwpZLjOzTkdS9hMKtunIVkNV7HinXiXb79QzM7N20pGJYCrwsbT10NFAta/om5m1v4JVDUlquKtzLyWv2/s6yeNgiYjJJI9OOIvkzsiNJHe0mplZOytYIkgf6tXc+AA+U6jlm9nuq6amhqqqKjZvbu7hqNYa3bt3p7KyktLS0pzn2ZNeI2dmnURVVRV9+vRhxIgRaKc3x1prRQSrV6+mqqqKkSNH5jyfHzFhZu1u8+bNDBgwwEkgzyQxYMCAXT7TciIwsw7hJFAYrVmvTgRmZkXOicDMilJJSQnjx49nzJgxXHjhhWzcuDHneRctWsRdd93V8oSNePe7392q+Rorw5gxOz3GrVWcCMysKPXo0YNZs2Yxd+5cysrKmDx58g7j6+qafiFYc4mgtrb5F7w988wzu17YAnMiMLOid9xxxzF//nymTZvGiSeeyEUXXcRhhx1GXV0dX/ziFznyyCMZO3YsP//5zwG49tprefLJJxk/fjw33HADt99+OxdeeCHnnHMOp512GuvXr+fkk0/m8MMP57DDDuOBBx7YtqzevZPXSk+bNo0TTjiBD3zgAxx88MFcfPHFNDwNeubMmUyaNIkjjjiC008/nTfffHPb8HHjxnHMMcdw00035e3zu/momXWob/xxHi8tW5vXmKOH9OXr5xya07S1tbU8/PDDnHHGGQA899xzzJ07l5EjRzJlyhTKy8t5/vnn2bJlC8ceeyynnXYa3/3ud/n+97/Pgw8mrw6//fbbmT59OnPmzKF///7U1tZy33330bdvX1atWsXRRx/Nueeeu9OF3BdeeIF58+YxZMgQjj32WJ5++mmOOuoorrnmGh544AEGDhzI3XffzXXXXcett97KZZddxo033sikSZP44hfz9+pnJwIzK0qbNm1i/PjxQHJGcPnll/PMM88wceLEbW3wH330UebMmcO9994LQHV1Na+99hplZWU7xTv11FPp378/kLTn/8pXvsITTzxBly5dWLp0KW+99Rb77LPPDvNMnDiRyspKAMaPH8+iRYuoqKhg7ty5nHrqqUBSRTV48GCqq6tZs2YNkyZNAuCSSy7h4Ycfzsu6cCIwsw6V65F7vjVcI8jWq1evbd0RwY033sjpp5++wzTTpk1rdr4777yTlStXMnPmTEpLSxkxYkSjbfu7deu2rbukpITa2loigkMPPZTp06fvMO2aNWsK1uTW1wjMzJpw+umnc/PNN1NTUwPAq6++yoYNG+jTpw/r1q1rcr7q6moGDRpEaWkpjz32GIsXN/kE6J2MGjWKlStXbksENTU1zJs3j4qKCsrLy3nqqaeAJNnki88IzMyacMUVV7Bo0SIOP/xwIoKBAwdy//33M3bsWLp27cq4ceO49NJL6dev3w7zXXzxxZxzzjlMmDCB8ePHsysv0yorK+Pee+/ls5/9LNXV1dTW1vL5z3+eQw89lNtuu41PfOIT9OzZc6ezlLYo2DuLC8UvpjHb87388ssccsghHV2MTqux9StpZkRMaGx6Vw2ZmRU5JwIzsyLnRGBmHWJPq5beU7RmvToRmFm76969O6tXr3YyyLOG9xF07959l+ZzqyEza3eVlZVUVVWxcuXKji5Kp9PwhrJd4URgZu2utLR0l96gZYXlqiEzsyLnRGBmVuScCMzMipwTgZlZkXMiMDMrck4EZmZFzonAzKzIORGYmRU5JwIzsyLnRGBmVuScCMzMipwTgZlZkXMiMDMrck4EZmZFzonAzKzIORGYmRW5giYCSWdI+qek+ZKubWR8uaQ/SpotaZ6kywpZHjMz21nBEoGkEuAm4ExgNPARSaOzJvsM8FJEjANOAH4gqaxQZTIzs50V8oxgIjA/IhZExFbgd8B5WdME0EeSgN7A20BtActkZmZZCpkIhgJLMvqr0mGZfgocAiwDXgQ+FxH12YEkXSlphqQZftm1mVl+FTIRqJFhkdV/OjALGAKMB34qqe9OM0VMiYgJETFh4MCB+S6nmVlRK2QiqAKGZfRXkhz5Z7oM+EMk5gMLgYMLWCYzM8tSyETwPHCgpJHpBeAPA1OzpnkDOBlA0t7AKGBBActkZmZZuhYqcETUSroa+BNQAtwaEfMkXZWOnwx8E7hd0oskVUlfiohVhSqTmZntrGCJACAiHgIeyho2OaN7GXBaIctgZmbN853FZmZFzonAzKzIORGYmRU5JwIzsyLnRGBmVuScCMzMipwTgZlZkXMiMDMrck4EZmZFzonAzKzIORGYmRW5FhOBpP7tURAzM+sYuZwR/F3SPZLOSl8paWZmnUguieAgYApwCTBf0rclHVTYYpmZWXtpMRGkbw/7c0R8BLgC+DjwnKTHJR1T8BKamVlBtfg+AkkDgI+SnBG8BVxD8qax8cA9wMgCls/MzAoslxfTTAd+DbwvIqoyhs+QNLmJeczMbA+RSyIYFRHR2IiI+F6ey2NmZu0sl4vFj0qqaOiR1E/SnwpXJDMza0+5JIKBEbGmoSci3gEGFaxEZmbWrnJJBHWShjf0SNoXaLSqyMzM9jy5XCO4DnhK0uNp//HAlYUrkpmZtacWE0FEPCLpcOBoQMAXImJVwUtmZmbtIpczAoA6YAXQHRgtiYh4onDFMjOz9pLLDWVXAJ8DKoFZJGcG04GTCloyMzNrF7lcLP4ccCSwOCJOBN4FrCxoqczMrN3kkgg2R8RmAEndIuIVYFRhi2VmZu0ll2sEVekNZfcDf5b0DrCskIUyM7P2k0urofPTzuslPQaUA48UtFRmZtZumk0EkroAcyJiDEBEPN7c9GZmtudp9hpBRNQDszPvLDYzs84ll2sEg4F5kp4DNjQMjIhzC1YqMzNrN7kkgm8UvBRmZtZhcrlY7OsCZmadWIv3EUhaJ2lt+rdZUp2ktbkEl3SGpH9Kmi/p2iamOUHSLEnzMh5sZ2Zm7SSXM4I+mf2S3gdMbGk+SSXATcCpQBXwvKSpEfFSxjQVwM+AMyLiDUl+z4GZWTvL5c7iHUTE/eT2nKGJwPyIWBARW4HfAedlTXMR8IeIeCONvWJXy2NmZm2Ty0PnLsjo7QJMILcX0wwFlmT0VwFHZU1zEFAqaRrQB/hxRNzRSBmuJH0HwvDhbslqZpZPubQaOiejuxZYxM5H9o1RI8OyE0hX4AjgZKAHMF3SsxHx6g4zRUwBpgBMmDDBb0czM8ujXK4RXNbK2FXAsIz+SnZ+RlEVsCoiNgAbJD0BjANexczM2kUurYZ+lV7UbejvJ+nWHGI/DxwoaaSkMuDDwNSsaR4AjpPUVVJPkqqjl3MuvZmZtVkuVUNjI2JNQ09EvCPpXS3NFBG1kq4G/gSUALdGxDxJV6XjJ0fEy5IeAeYA9cAtETG3NR/EzMxaJ5dE0EVSv4h4B0BS/xznIyIeAh7KGjY5q/9/gP/JrbhmZpZvuezQfwA8I+lekou9HwT+q6ClMjOzdpPLxeI7JM0guXdAwAWZN4WZmdmeLZf7CI4G5kXET9P+PpKOioi/F7x0ZmZWcLncWXwzsD6jf0M6zMzMOoFcEoEiYttNXOnLanK6WGxmZru/XBLBAkmflVSa/n0OWFDogpmZWfvIJRFcBbwbWMr25wV9spCFMjOz9pNLq6EVJHcFAyCpB3A2cE8By2VmZu0kp8dQSyqRdKakO4CFwIcKWywzM2svzZ4RSDqe5J0B7wWeA44F9ouIje1QNjMzawdNJgJJVcAbJE1FvxgR6yQtdBIwM+tcmqsa+j3Jy2U+BJwjqRe5vZDGzMz2IE0mgoj4HDAC+CFwIsk7AgZK+qCk3u1TPDMzK7RmLxZH4m8R8UmSpHAR8D6St5SZmVknkPMdwhFRA/wR+GPahNTMzDqBVj0qIiI25bsgZma7u9q6etZurmXNxq2s2VRD9cYa1mzamv6vYc3GGqo31RARlPcopbxnGRU9SqnomfyV9yilvEfZtu7Skpxa8BecnxlUROrrg2XVm6joWUbvbv7q823V+i307V5KWdfdY+O2xkUEm2vqqd6U7MTXbEx24Gsz+9OdfOY01RtrWLelttnYfbp3paJnKcC26aOZJja9ykqo6FlGeRPJoiGJ9O1RSkWPMgaXd6dfr7J8rg7AiaDTigiWr93M7CXVzK5aw5yqNcypqmbd5uSHvFfvMob378mIAb0YPiD5v++Anuw7oBf9epYiqYM/wZ5jzcatfOehV7h7xhLKunbhkMF9GV9ZztjKCsYNq2C/vXrRpYvXZ77V1wfrNtdu31FvatiB79ifdG9N+tOd/Nba+ibjdu2ijB1yKYP6dOegQX0o75nsjMt7dE123tt21MmOvG/3rnTNOsKvqw/Wbd6+3DUbk3JsK0tazur0M7z61vpt5a2p2zmDfOr4/fjyWYfkfV0qmktXgKSDgC8C+5KROCLipLyXJgcTJkyIGTNmdMSid2trNm5lTlU1s5esYXZVsvNfuW4LkPywDx7ch7GVFYwe3Jd1m2tZvHoDi1Zv4I3VG1lWvXmHWH26d92WFPbNShaD+nTzTi0VEdw/aynfevBl1myq4ZKj96WsaxdmLVnD3KXVbNxaB0Cfbl0ZM7ScccMqGFdZzthhFQwp7+5k24KNW2uZu3Qts5es4aU317J6w9ZkJ5/uRNdurmn2aLtnWQkVGdUzDUfd23foybB+6bDydKfeq6ykw7+biGBTTd1OyWJ4/16MHtK3VTElzYyICY2Ny+WM4B5gMvALoK5VJbC82rS1jrnLkp3+nHSnv3j19vv89hvYi/ccsNe2nc7owX3pXlrSZLzNNXVUvbORRas2svjtjSxevYHFqzcyb2k1f5q7nNr67Vtb99IuDO+/PUnsu1fyf0hFD7oWIEH0711G3+6leY/bVgtXbeCr97/I0/NXM35YBb+54DAOGbx9A62rD15fuZ5ZS7afjf3yqQXbjvL26t0t+X4qKxg3rJxxlRUFOeXfU2ytreefy9dtO3udvaSa11aso+GnN7i8O4P6dqeiZxkj9uqV7LSz6uC37ejTnfyeXEUniZ5lXelZ1pUhFYVvm5PLGcHMiDii4CXJUbGdEdTUJRvI9qP9Nby2Yj116RYypLw7YysrGDusnPGVFYypLM/rjrO2rp5lazaz+O0NLFq9kcWrNmxLFm+8vZHNNU2fYudD99IuXDRxXz41aT/27tu9oMvKxZbaOqY8voAbH5tPt5Iu/PuZB3PRxOGU5JAEN9fU8cryddu+xzlV1by+cv22o9ph/XswrrIi+RtWwZihfelZ1vlqb+vrgwWrNqQ7/OQM9qU3126rrunfq4yxaZIcPyz5v1fvbh1c6j1fc2cEuSSC64EVwH3AlobhEfF2HsuYs86eCCKCecvW8uCcN3lu4WrmLVvLlnQDqehZmmwc6UYydlg5g/p03M6xvj5YsW4Li1dvYPnazdS38FvaVRHw1PxVPDBrGSUSHzyykqsm7U9lv555XU6unlv4Nl+570Xmr1jPe8cO5utnj2ZQG5PTus01vLi0mjlV1duOhJeuSRrldREcOKgPo/bpQ9eS/J9tlZV02VZNsu2IukdadZIeafdsYzVJRPBm9eZtO/w5VWt4sap620XXnmUljBlazvhhFYytTM6MKvv16PCqmc6orYlgYSODIyL2y0fhdlVnTQSvr1zP1FnL+OOcZSxYuYGuXcS7hidHh2OHVTC+soJh/YtzA3lj9UZufnw+986sIgIuOHwonz7hAEbs1atdlp95MXhoRQ++9b4xnHjwoIItb+W6Lby4dA2zliQ7zgUrN+Q9yQJsqa2nemMNW+tyu3C6rXVLj9KdEkjDhdO+PUpZ8vbGbYlt1pJqVq1Pjh9LS8Qhg/tmHO1XsP/A3jmdTVnbtSkR7G46UyJYtmYTf5y9jKmzlzFv2VokOGpkf84dN5Qzx+xT1HXGjVm6ZhNTHn+d3z6/hNq6es4bP5TPnLg/BwzqU5DlZV8MvuK4kXzu5AM7VXVNQ1PKHVrebNzeimVN2sKlob38tguXLTSllGD/gb0ZW9lwtF/BIYP70K1r09eqrLDaekZQCvwLcHw6aBrw8/RO43a3pyeC1eu38NCLbzJ19jKeX/QOAOOGVXDO2MGcPXYI+5R3fD347m7F2s384skF/ObZN9hcW8eZY/bh6hMPbHVrisZkXwz+9vmH5TV+Z1BTV8/aTZnNNpNksXff7hw2tJw+u+FF/mLW1kRwC1AK/CoddAlQFxFX5LWUOdoTE8HazTU8Ou8tps5extPzV1FXHxy0d2/OHTeEc8YNYd8B7VPF0dm8vWErv3xqAb96ZjHrt9RyyiF7c81JBzBuWEWrY7blYrDZ7qytiWB2RIxraVh72VMSweaaOv768gqmzl7KY/9cydbaeir79eDccUM4d/wQDt7HR5f5Ur2xhtufWcStTy+kelMNxx80kGtOOoAjR/TfpTjZF4O/dvbo3aKlklk+tPU+gjpJ+0fE62mw/fD9BI2qqavnqddWMXX2Mh6dt5wNW+sY2KcbF00czrnjh/CuYRVFebG30Mp7lvK5Uw7k8uNG8uvpi7nlyQVcOHk6R43sz2dPPpB37z+g2fWefTH4tkuPLOjFYLPdTS5nBCcDtwELAJHcYXxZRDxW+OLtbHc7I6ivD55b9DZTZy/j4Rff5J2NNZT3KOXMMftwzrghHL3fAFcrtLNNW+u467k3+Pnjr7Ni3RYOH17BNScdyAmjBu6QEIrhYrBZgza3GpLUDRhFkgheiYgtLcxSMLtLIqitq+ePc5bx07/N5/WVG+hRWsKpo/fm3HFDOP6ggXv0XY2dxeaaOu6ZWcXkaa+zdM0mxgzty9UnHshpo/dm8dsbfTHYikqrEoGkkyLib5IuaGx8RPwhj2XMWUcngq219dz3QhU/m/Y6i1dv5OB9+vCpSftx+qH7+EhyN1VTV899/1jKTdPms3j1RvYb2Iuqdzb5YrAVldZeI5gE/A04p5FxAXRIIugom2vquGfGEiY/voClazYxtrKcKZccwSmH7O2HsO3mSku68MEjh3HB4UP5vxff5LanF3HGoeVc995DfDHYjNyuEYyMiIUtDWsv7X1GsHFrLXf9/Q2mPLGAFeu2cMS+/bjmpAOYdNBAX/g1sz1GW1sN/R44PGvYvcBu8yC6Qli3uYY7pi/ml08t5O0NW3n3/gP40YfHc8x+zbdAMTPb0zSZCCQdDBwKlGddJ+gLdNrz6eqNNdz69EJue3ohazfXcsKopE36EfvuWpt0M7M9RXNnBKOAs4EKdrxOsA74ZC7BJZ0B/BgoAW6JiO82Md2RwLPAhyLi3lxi59vq9Vu45amF/Hp6cpfqaaP35pqTDuSwyvKOKI6ZWbtpMhFExAPAA5KOiYjpuxpYUglwE3AqUAU8L2lqRLzUyHTfA/60q8vIhxVrN/PzJxZw598Xs6W2nvceNpirTzrAd/6aWdHI5RrBC5I+Q1JNtK1KKCI+0cJ8E4H5EbEAQNLvgPOAl7Kmu4bkOsSRuRY6H5au2cTkaa9z94wl1NUH540fwmdOPID9B/Zuz2KYmXW4XBLBr4FXgNOB/wQuBl7OYb6hwJKM/irgqMwJJA0FzgdOoplEIOlK4EqA4cOH57Dopi1evYGfPfY6v/9HFRJ84IhK/mXSAQwf0DEvOzEz62i5JIIDIuJCSedFxK8k3UVu1TiNNa3Jbqv6I+BLEVHXXEuciJgCTIGk+WgOy97JwlUb+MlfX+OBWUvpWtKFi48azqcm7d8u7wM1M9ud5ZIIGt47sEbSGGA5MCKH+aqAYRn9lcCyrGkmAL9Lk8BewFmSaiPi/hzi75KFq9bzyNzlXP6ekXzyuP3a/IpBM7POIpdEMEVSP+A/gKlAb+BrOcz3PHCgpJHAUuDDwEWZE0TEyIZuSbcDDxYiCQCcOGoQz1x7kt/6ZWaWpcVEEBG3pJ2PAzm/pzgiaiVdTVKNVALcGhHzJF2Vjp/civK2miQnATOzRjR3Q9n/a27GiPhhS8Ej4iHgoaxhjSaAiLi0pXhmZpZ/zZ0RNLwRfBRJi56paf85wBOFLJSZmbWf5m4o+waApEeBwyNiXdp/PXBPu5TOzMwKLpe3pwwHtmb0byW3VkNmZrYHyPWGsuck3UdyH8D5wB0FLZWZmbWbXFoN/Zekh4Hj0kGXRcQLhS2WmZm1l+ZaDfWNiLWS+gOL0r+Gcf0j4u3CF8/MzAqtuTOCu0geQz2THR8NobQ/53sKzMxs99Vcq6Gz0/8jm5rGzMz2fM1VDWW/nnIHEfGP/BfHzMzaW3NVQz9oZlyQPDrazMz2cM1VDZ3YngUxM7OOkct9BKSPnx7Njm8o870EZmadQIuJQNLXgRNIEsFDwJnAU/imMjOzTiGXR0x8ADgZWB4RlwHjgG4FLZWZmbWbXBLBpoioB2ol9QVW4HsIzMw6jVyuEcyQVAH8guTmsvXAc4UslJmZtZ/m7iP4KXBXRHw6HTRZ0iNA34iY0y6lMzOzgmvujOA14AeSBgN3A7+NiFntUiozM2s3TV4jiIgfR8QxwCTgbeA2SS9L+pqkg9qthGZmVlAtXiyOiMUR8b2IeBdwEcn7CF4ueMnMzKxdtJgIJJVKOkfSncDDwKvA+wteMjMzaxfNXSw+FfgI8F6SVkK/A66MiA3tVDYzM2sHzV0s/grJOwn+zS+hMTPrvPzQOTOzIpfLncVmZtaJORGYmRU5JwIzsyLnRGBmVuScCMzMipwTgZlZkXMiMDMrck4EZmZFzonAzKzIORGYmRW5giYCSWdI+qek+ZKubWT8xZLmpH/PSBpXyPKYmdnOCpYIJJUANwFnAqOBj0ganTXZQmBSRIwFvglMKVR5zMyscYU8I5gIzI+IBRGxleQx1udlThARz0TEO2nvs0BlActjZmaNKGQiGAosyeivSoc15XKSF9/sRNKVkmZImrFy5co8FtHMzAqZCNTIsGh0QulEkkTwpcbGR8SUiJgQERMGDhyYxyKamVlzL6ZpqypgWEZ/JbAseyJJY4FbgDMjYnUBy2NmZo0o5BnB88CBkkZKKgM+DEzNnEDScOAPwCUR8WoBy2JmZk0o2BlBRNRKuhr4E1AC3BoR8yRdlY6fDHwNGAD8TBJAbURMKFSZzMxsZ4potNp+tzVhwoSYMWNGRxfDzGyPImlmUwfavrPYzKzIORGYmRU5JwIzsyLnRGBmVuScCMzMipwTgZlZkXMiMDMrck4EZmZFzonAzKzIORGYmRU5JwIzsyLnRGBmVuScCMzMipwTgZlZkXMiMDMrck4EZmZFzonAzKzIORGYmRU5JwIzsyLnRGBmVuScCMzMipwTgZlZkXMiMDMrck4EZmZFzonAzKzIORGYmRU5JwIzsyLnRGBmVuScCMzMipwTgZlZkXMiMDMrck4EZmZFzonAzKzIORGYmRU5JwIzsyJX0EQg6QxJ/5Q0X9K1jYyXpJ+k4+dIOryQ5TEzs50VLBFIKgFuAs4ERgMfkTQ6a7IzgQPTvyuBmwtVHjMza1whzwgmAvMjYkFEbAV+B5yXNc15wB2ReBaokDS4gGUyM7MsXQsYeyiwJKO/Cjgqh2mGAm9mTiTpSpIzBoD1kv7ZyjLtBaxq5byO2/4xHbdwMR23cDF317j7NjWikIlAjQyLVkxDREwBprS5QNKMiJjQ1jiO2z4xHbdwMR23cDH3xLiFrBqqAoZl9FcCy1oxjZmZFVAhE8HzwIGSRkoqAz4MTM2aZirwsbT10NFAdUS8mR3IzMwKp2BVQxFRK+lq4E9ACXBrRMyTdFU6fjLwEHAWMB/YCFxWqPKk2ly95LjtGtNxCxfTcQsXc4+Lq4idquTNzKyI+M5iM7Mi50RgZlbkiiIRSLpV0gpJc/Mcd5ikxyS9LGmepM/lIWZ3Sc9Jmp3G/EY+ypoRv0TSC5IezGPMRZJelDRL0ow8xq2QdK+kV9J1fEwb441Ky9jwt1bS5/NU1i+k39dcSb+V1D1PcT+XxpzXlrI2tg1I6i/pz5JeS//3y1PcC9Py1kva5aaOTcT8n/R3MEfSfZIq8hT3m2nMWZIelTQkH3Ezxv2bpJC0V57Ke72kpRm/4bN2NW6jIqLT/wHHA4cDc/McdzBweNrdB3gVGN3GmAJ6p92lwN+Bo/NY5v8H3AU8mMeYi4C9CvC9/Qq4Iu0uAyryGLsEWA7sm4dYQ4GFQI+0/3+BS/MQdwwwF+hJ0rDjL8CBrYy10zYA/Ddwbdp9LfC9PMU9BBgFTAMm5CnmaUDXtPt7eSxr34zuzwKT8xE3HT6MpLHM4tZsH02U93rg39r628r+K4ozgoh4Ani7AHHfjIh/pN3rgJdJdgptiRkRsT7tLU3/8nJFX1Il8F7glnzEKyRJfUk2hF8CRMTWiFiTx0WcDLweEYvzFK8r0ENSV5Iddz7uhzkEeDYiNkZELfA4cH5rAjWxDZxHkmxJ/78vH3Ej4uWIaO3d/03FfDRdBwDPktxzlI+4azN6e9GKba2Z/csNwL+3JmYLcfOuKBJBe5A0AngXyRF8W2OVSJoFrAD+HBFtjpn6EckPsz5P8RoE8KikmenjQPJhP2AlcFtalXWLpF55ig3JfS2/zUegiFgKfB94g+TxKNUR8WgeQs8Fjpc0QFJPkqbWw1qYZ1fsHel9O+n/QXmMXUifAB7OVzBJ/yVpCXAx8LU8xTwXWBoRs/MRL8vVaXXWra2pzmuME0EeSOoN/B74fNYRRqtERF1EjCc56pkoaUxbY0o6G1gRETPbGqsRx0bE4SRPk/2MpOPzELMryWnxzRHxLmADSfVFm6U3OJ4L3JOneP1Ijq5HAkOAXpI+2ta4EfEySTXIn4FHgNlAbbMzdXKSriNZB3fmK2ZEXBcRw9KYV7c1Xpq0ryNPSSXLzcD+wHiSg44f5COoE0EbSSolSQJ3RsQf8hk7rQqZBpyRh3DHAudKWkTyJNiTJP0mD3GJiGXp/xXAfSRPnm2rKqAq42zoXpLEkA9nAv+IiLfyFO8UYGFErIyIGuAPwLvzETgifhkRh0fE8STVBK/lI27qLaVP+03/r8hj7LyT9HHgbODiSCvM8+wu4P15iLM/yUHB7HR7qwT+IWmftgaOiLfSA8V64BfkZ1tzImgLSSKpw345In6Yp5gDG1pESOpBspN5pa1xI+LLEVEZESNIqkX+FhFtPmqV1EtSn4Zukot6bW6dFRHLgSWSRqWDTgZeamvc1EfIU7VQ6g3gaEk909/EySTXi9pM0qD0/3DgAvJb7qnAx9PujwMP5DF2Xkk6A/gScG5EbMxj3AMzes8lP9vaixExKCJGpNtbFUmjkuVtja0dH9N/PnnY1oCiaTX0W5LTqBqSL+XyPMV9D0n9+BxgVvp3VhtjjgVeSGPOBb5WgPVxAnlqNURSlz87/ZsHXJfHco4HZqTr4n6gXx5i9gRWA+V5XqffINmJzAV+DXTLU9wnSRLgbODkNsTZaRsABgB/JTnL+CvQP09xz0+7twBvAX/KQ8z5JI+sb9jOWtO6p7G4v0+/sznAH4Gh+YibNX4RrWs11Fh5fw28mJZ3KjA4H78zP2LCzKzIuWrIzKzIORGYmRU5JwIzsyLnRGBmVuScCMzMipwTge1R0sctNDx5cXnWkxjLWph3gqSf5LCMZ/JU1hMkVWc98fSUfMRO418q6af5imfFq2CvqjQrhIhYTXJ/AZKuB9ZHxPcbxkvqGtsfTpY97wyS+xJaWkZe7gpOPRkRZ+cxnlne+YzA9niSbpf0Q0mPAd+TNFHSM+nD6p5puDs5PUJ/MO2+Pn1o1zRJCyR9NiPe+ozpp2n7OxHuTO8cRtJZ6bCnJP1Eu/B+B0kj0nl/lT487N70+TRIOjkt94tp+bqlw49MP8tsJe+r6JOGGyLpESXvFPjvdNqSdJ3MTeN8oe1r2ToznxFYZ3EQcEpE1Cl9hHVE1KZVMd+m8WfIHAycSPIuiX9KujmSZwVlehdwKMljpZ8GjlXy8p2fp8tYKKm5xz4cp+RJsg3eD9SRPK//8oh4WtKtwKfTap7bSe4gflXSHcC/SPoZcDfwoYh4Pv18m9J449Mybkk/w40kTxEdGhFjIHnBTzPlM/MZgXUa90REXdpdDtyj5M1ON5DsyBvzfxGxJSJWkTxwbe9GpnkuIqoiecjXLGAESQJZEBEL02maSwRPRsT4jL/X0+FLIuLptPs3JI8rGUXy8LpX0+G/InknwyjgzYh4HpJn6GdUf/01IqojYjPJoyj2BRYA+0m6MX1GT5ufiGudmxOBdRYbMrq/CTyWHhGfAzT12sgtGd11NH6G3Ng0akM5G2Q/2yWaiatGpm+wU/ki4h1gHMmTaz/DHvAiIutYTgTWGZUDS9PuSwsQ/xWSI+4Raf+HWhFjuLa/g/kjwFNp3BGSDkiHX0LyVrJXSK4FHAkgqY+SN6E1Ssn7cbtExO+B/yB/j++2TsqJwDqj/wa+I+lpkncT51VEbAI+DTwi6SmSJ2xWNzH5cVnNRz+QDn8Z+LikOUB/khfwbAYuI6nWepHkTXKTI2IrSbK5UdJskhfVNHWWA8nrUqel1yZuB77cho9rRcBPHzVrBUm9I2J92oroJuC1iLghx3lHkDwGvM1vnjPLB58RmLXOJ9Mj7nkkVVE/79jimLWezwjMzIqczwjMzIqcE4GZWZFzIjAzK3JOBGZmRc6JwMysyP1/yIo/SO2zhiUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ohist = [h.cpu().numpy() for h in hist]\n",
    "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights.\n"
     ]
    }
   ],
   "source": [
    "model_ft = ViT(model_name, pretrained=True)\n",
    "set_parameter_requires_grad(model_ft, feature_extracting=True, blocks=[])\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = torch.nn.Linear(num_ftrs, train_dataset.num_labels)\n",
    "input_size = 384\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "\n",
    "# TODO: try unfreezing the last few layers/blocks as well\n",
    "print(\"Params to learn:\")\n",
    "params_to_update = []\n",
    "for name,param in model_ft.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\",name)\n",
    "\n",
    "\n",
    "# Try the original transformer's optimizer\n",
    "optimizer_ft = transformers.AdamW(params_to_update, lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 1.9543 Acc: 0.2370\n",
      "val Loss: 1.8218 Acc: 0.3125\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 1.7780 Acc: 0.3281\n",
      "val Loss: 1.7484 Acc: 0.3264\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 1.7208 Acc: 0.3403\n",
      "val Loss: 1.7043 Acc: 0.3472\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 1.6783 Acc: 0.3394\n",
      "val Loss: 1.6752 Acc: 0.3681\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 1.6309 Acc: 0.3646\n",
      "val Loss: 1.6496 Acc: 0.3750\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 1.6042 Acc: 0.3811\n",
      "val Loss: 1.6337 Acc: 0.3819\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 1.6000 Acc: 0.3733\n",
      "val Loss: 1.6125 Acc: 0.3889\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 1.5709 Acc: 0.4028\n",
      "val Loss: 1.6020 Acc: 0.3889\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 1.5628 Acc: 0.4002\n",
      "val Loss: 1.5899 Acc: 0.4028\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 1.5287 Acc: 0.4201\n",
      "val Loss: 1.5789 Acc: 0.3889\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 1.5194 Acc: 0.4158\n",
      "val Loss: 1.5803 Acc: 0.3958\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 1.5196 Acc: 0.4158\n",
      "val Loss: 1.5605 Acc: 0.4028\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 1.5090 Acc: 0.4288\n",
      "val Loss: 1.5596 Acc: 0.3958\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 1.4892 Acc: 0.4288\n",
      "val Loss: 1.5523 Acc: 0.4097\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 1.4615 Acc: 0.4497\n",
      "val Loss: 1.5415 Acc: 0.4167\n",
      "\n",
      "Training complete in 31m 23s\n",
      "Best val Acc: 0.416667\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "train_dataset = TransformerTopCropDataset(train)\n",
    "val_dataset = TransformerTopCropDataset(val)\n",
    "test_dataset = TransformerTopCropDataset(test)\n",
    "dataloaders_dict = {'train': torch.utils.data.DataLoader(train_dataset), \n",
    "                    'val': torch.utils.data.DataLoader(val_dataset), \n",
    "                    'test': torch.utils.data.DataLoader(test_dataset)}\n",
    "# Train and evaluate\n",
    "num_epochs = 15\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft, \"visual_transformer_mel_topcrop_stacked_freeze_layers_lr5e-5_AdamW.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 1.4472 Acc: 0.4583\n",
      "val Loss: 1.5340 Acc: 0.4306\n",
      "\n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 1.4417 Acc: 0.4505\n",
      "val Loss: 1.5296 Acc: 0.4167\n",
      "\n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.4451 Acc: 0.4670\n",
      "val Loss: 1.5283 Acc: 0.3958\n",
      "\n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 1.4192 Acc: 0.4627\n",
      "val Loss: 1.5158 Acc: 0.4097\n",
      "\n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 1.4220 Acc: 0.4818\n",
      "val Loss: 1.5161 Acc: 0.4097\n",
      "\n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 1.4123 Acc: 0.4774\n",
      "val Loss: 1.5130 Acc: 0.4167\n",
      "\n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 1.3940 Acc: 0.4939\n",
      "val Loss: 1.5100 Acc: 0.4236\n",
      "\n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 1.4011 Acc: 0.4731\n",
      "val Loss: 1.5103 Acc: 0.4097\n",
      "\n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 1.3918 Acc: 0.4714\n",
      "val Loss: 1.5046 Acc: 0.4097\n",
      "\n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 1.3780 Acc: 0.4861\n",
      "val Loss: 1.5052 Acc: 0.4097\n",
      "\n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 1.3786 Acc: 0.4878\n",
      "val Loss: 1.4961 Acc: 0.4167\n",
      "\n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 1.3631 Acc: 0.5104\n",
      "val Loss: 1.4905 Acc: 0.3958\n",
      "\n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 1.3559 Acc: 0.4957\n",
      "val Loss: 1.4915 Acc: 0.4097\n",
      "\n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 1.3508 Acc: 0.5035\n",
      "val Loss: 1.4934 Acc: 0.4167\n",
      "\n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 1.3458 Acc: 0.5035\n",
      "val Loss: 1.4956 Acc: 0.4167\n",
      "\n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 1.3294 Acc: 0.5269\n",
      "val Loss: 1.4954 Acc: 0.4167\n",
      "\n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 1.3388 Acc: 0.5026\n",
      "val Loss: 1.4963 Acc: 0.4236\n",
      "\n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 1.3307 Acc: 0.5104\n",
      "val Loss: 1.4918 Acc: 0.4306\n",
      "\n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 1.3341 Acc: 0.5095\n",
      "val Loss: 1.4858 Acc: 0.4167\n",
      "\n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 1.3035 Acc: 0.5148\n",
      "val Loss: 1.4872 Acc: 0.4167\n",
      "\n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 1.3152 Acc: 0.5130\n",
      "val Loss: 1.4843 Acc: 0.4306\n",
      "\n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 1.3001 Acc: 0.5286\n",
      "val Loss: 1.4809 Acc: 0.4097\n",
      "\n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 1.3143 Acc: 0.5217\n",
      "val Loss: 1.4821 Acc: 0.4306\n",
      "\n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 1.2920 Acc: 0.5165\n",
      "val Loss: 1.4805 Acc: 0.4236\n",
      "\n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 1.2930 Acc: 0.5382\n",
      "val Loss: 1.4808 Acc: 0.4306\n",
      "\n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 1.2995 Acc: 0.5208\n",
      "val Loss: 1.4683 Acc: 0.4306\n",
      "\n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 1.2903 Acc: 0.5286\n",
      "val Loss: 1.4758 Acc: 0.4375\n",
      "\n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 1.2916 Acc: 0.5217\n",
      "val Loss: 1.4783 Acc: 0.4236\n",
      "\n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 1.2776 Acc: 0.5330\n",
      "val Loss: 1.4684 Acc: 0.4306\n",
      "\n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 1.2551 Acc: 0.5538\n",
      "val Loss: 1.4728 Acc: 0.4306\n",
      "\n",
      "Training complete in 62m 33s\n",
      "Best val Acc: 0.437500\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "model_ft = ViT(model_name, pretrained=True)\n",
    "set_parameter_requires_grad(model_ft, feature_extracting=True, blocks=[])\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = torch.nn.Linear(num_ftrs, train_dataset.num_labels)\n",
    "input_size = 384\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ft = model_ft.to(device)\n",
    "\"\"\"\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "train_dataset = TransformerTopCropDataset(train)\n",
    "val_dataset = TransformerTopCropDataset(val)\n",
    "test_dataset = TransformerTopCropDataset(test)\n",
    "dataloaders_dict = {'train': torch.utils.data.DataLoader(train_dataset), \n",
    "                    'val': torch.utils.data.DataLoader(val_dataset), \n",
    "                    'test': torch.utils.data.DataLoader(test_dataset)}\n",
    "# continue training above model for 30 epochs\n",
    "num_epochs = 30\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft, \"visual_transformer_mel_topcrop_stacked_freeze_layers_lr5e-5_AdamW_45epoch.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights.\n"
     ]
    }
   ],
   "source": [
    "model_ft = ViT(model_name, pretrained=True)\n",
    "set_parameter_requires_grad(model_ft, feature_extracting=True, blocks=[11])\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = torch.nn.Linear(num_ftrs, train_dataset.num_labels)\n",
    "input_size = 384\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t transformer.blocks.11.attn.proj_q.weight\n",
      "\t transformer.blocks.11.attn.proj_q.bias\n",
      "\t transformer.blocks.11.attn.proj_k.weight\n",
      "\t transformer.blocks.11.attn.proj_k.bias\n",
      "\t transformer.blocks.11.attn.proj_v.weight\n",
      "\t transformer.blocks.11.attn.proj_v.bias\n",
      "\t transformer.blocks.11.proj.weight\n",
      "\t transformer.blocks.11.proj.bias\n",
      "\t transformer.blocks.11.norm1.weight\n",
      "\t transformer.blocks.11.norm1.bias\n",
      "\t transformer.blocks.11.pwff.fc1.weight\n",
      "\t transformer.blocks.11.pwff.fc1.bias\n",
      "\t transformer.blocks.11.pwff.fc2.weight\n",
      "\t transformer.blocks.11.pwff.fc2.bias\n",
      "\t transformer.blocks.11.norm2.weight\n",
      "\t transformer.blocks.11.norm2.bias\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "\n",
    "# TODO: try unfreezing the last few layers/blocks as well\n",
    "print(\"Params to learn:\")\n",
    "params_to_update = []\n",
    "for name,param in model_ft.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\",name)\n",
    "\n",
    "\n",
    "# Try the original transformer's optimizer\n",
    "optimizer_ft = transformers.AdamW(params_to_update, lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/29\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-79661d0e3d71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Train and evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_inception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"inception\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-c294d2957ad0>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders, criterion, optimizer, num_epochs, is_inception)\u001b[0m\n\u001b[1;32m     46\u001b[0m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_pretrained_vit/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'positional_embedding'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# b,gh*gw+1,d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# b,gh*gw+1,d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pre_logits'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_pretrained_vit/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_pretrained_vit/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpwff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_pretrained_vit/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# (B, S, D) -> (B, S, D_ff) -> (B, S, D)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "train_dataset = TransformerTopCropDataset(train)\n",
    "val_dataset = TransformerTopCropDataset(val)\n",
    "test_dataset = TransformerTopCropDataset(test)\n",
    "dataloaders_dict = {'train': torch.utils.data.DataLoader(train_dataset),#, batch_size=1), \n",
    "                    'val': torch.utils.data.DataLoader(val_dataset),#, batch_size=1), \n",
    "                    'test': torch.utils.data.DataLoader(test_dataset)}#, batch_size=1)}\n",
    "# Train and evaluate\n",
    "num_epochs = 30\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
