{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import transformers\n",
    "from pytorch_pretrained_vit import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'B_16_imagenet1k'\n",
    "model = ViT(model_name, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/preprocessed_data_split_nona_03_07.pkl', 'rb') as f:\n",
    "    train, val, test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>intensity</th>\n",
       "      <th>statement</th>\n",
       "      <th>repeat</th>\n",
       "      <th>gender</th>\n",
       "      <th>mel</th>\n",
       "      <th>mfcc</th>\n",
       "      <th>chromagram</th>\n",
       "      <th>spec_contrast</th>\n",
       "      <th>tonnetz</th>\n",
       "      <th>filename</th>\n",
       "      <th>mel_pad</th>\n",
       "      <th>mfcc_pad</th>\n",
       "      <th>chromagram_pad</th>\n",
       "      <th>spec_contrast_pad</th>\n",
       "      <th>tonnetz_pad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[-976.11755, -976.11755, -976.11755, -976.117...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[18.698652440717126, 18.698652440717126, 18.6...</td>\n",
       "      <td>[[-0.24351785481696392, -0.22269760507170488, ...</td>\n",
       "      <td>03-01-02-01-02-02-20.wav</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[-976.1175537109375, -976.1175537109375, -976...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[18.698652440717126, 18.698652440717126, 18.6...</td>\n",
       "      <td>[[-0.24351785481696392, -0.22269760507170488, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[-919.54193, -919.54193, -919.54193, -919.541...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[14.30063620727286, 14.30063620727286, 14.300...</td>\n",
       "      <td>[[0.011945099331460431, 0.02484843939140515, 0...</td>\n",
       "      <td>03-01-06-01-01-01-14.wav</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[-919.5419311523438, -919.5419311523438, -919...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[14.30063620727286, 14.30063620727286, 14.300...</td>\n",
       "      <td>[[0.011945099331460431, 0.02484843939140515, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[[5.976571e-14, 3.2825318e-09, 4.7936957e-09, ...</td>\n",
       "      <td>[[-735.2311, -735.2311, -735.2311, -735.2311, ...</td>\n",
       "      <td>[[0.91999584, 0.9419208, 0.9717938, 1.0, 1.0, ...</td>\n",
       "      <td>[[27.160881999404914, 10.240644242524503, 17.3...</td>\n",
       "      <td>[[-0.039178584692475336, -0.07856553551372042,...</td>\n",
       "      <td>03-01-07-01-02-01-04.wav</td>\n",
       "      <td>[[5.976570963388966e-14, 3.282531801929167e-09...</td>\n",
       "      <td>[[-735.2310791015625, -735.2310791015625, -735...</td>\n",
       "      <td>[[0.9199958443641663, 0.941920816898346, 0.971...</td>\n",
       "      <td>[[27.160881999404914, 10.240644242524503, 17.3...</td>\n",
       "      <td>[[-0.039178584692475336, -0.07856553551372042,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[[5.161722e-06, 1.9711595e-06, 1.4549606e-07, ...</td>\n",
       "      <td>[[-760.2454, -761.35565, -763.5334, -763.5334,...</td>\n",
       "      <td>[[0.7726974, 0.86186564, 0.8380048, 0.6996503,...</td>\n",
       "      <td>[[25.352601414654497, 17.07070284635677, 10.08...</td>\n",
       "      <td>[[-0.09442284617849674, -0.08483387511976943, ...</td>\n",
       "      <td>03-01-05-01-02-01-06.wav</td>\n",
       "      <td>[[5.16172212883248e-06, 1.971159463209915e-06,...</td>\n",
       "      <td>[[-760.2454223632812, -761.3556518554688, -763...</td>\n",
       "      <td>[[0.772697389125824, 0.8618656396865845, 0.838...</td>\n",
       "      <td>[[25.352601414654497, 17.07070284635677, 10.08...</td>\n",
       "      <td>[[-0.09442284617849674, -0.08483387511976943, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[[2.1212953e-11, 1.1371189e-10, 2.6217887e-13,...</td>\n",
       "      <td>[[-766.1009, -766.1009, -766.1009, -766.1009, ...</td>\n",
       "      <td>[[0.8027966, 0.9662601, 0.93856305, 0.934313, ...</td>\n",
       "      <td>[[20.23687827597825, 13.043456325271165, 21.71...</td>\n",
       "      <td>[[0.15901379770687685, 0.10643275780873687, 0....</td>\n",
       "      <td>03-01-04-02-02-01-14.wav</td>\n",
       "      <td>[[2.1212953268956447e-11, 1.1371188712860913e-...</td>\n",
       "      <td>[[-766.1008911132812, -766.1008911132812, -766...</td>\n",
       "      <td>[[0.8027966022491455, 0.9662600755691528, 0.93...</td>\n",
       "      <td>[[20.23687827597825, 13.043456325271165, 21.71...</td>\n",
       "      <td>[[0.15901379770687685, 0.10643275780873687, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      emotion  intensity  statement  repeat  gender  \\\n",
       "195         2          1          2       2       0   \n",
       "1048        6          1          1       1       0   \n",
       "532         7          1          2       1       0   \n",
       "1273        5          1          2       1       0   \n",
       "1064        4          2          2       1       0   \n",
       "\n",
       "                                                    mel  \\\n",
       "195   [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1048  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "532   [[5.976571e-14, 3.2825318e-09, 4.7936957e-09, ...   \n",
       "1273  [[5.161722e-06, 1.9711595e-06, 1.4549606e-07, ...   \n",
       "1064  [[2.1212953e-11, 1.1371189e-10, 2.6217887e-13,...   \n",
       "\n",
       "                                                   mfcc  \\\n",
       "195   [[-976.11755, -976.11755, -976.11755, -976.117...   \n",
       "1048  [[-919.54193, -919.54193, -919.54193, -919.541...   \n",
       "532   [[-735.2311, -735.2311, -735.2311, -735.2311, ...   \n",
       "1273  [[-760.2454, -761.35565, -763.5334, -763.5334,...   \n",
       "1064  [[-766.1009, -766.1009, -766.1009, -766.1009, ...   \n",
       "\n",
       "                                             chromagram  \\\n",
       "195   [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1048  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "532   [[0.91999584, 0.9419208, 0.9717938, 1.0, 1.0, ...   \n",
       "1273  [[0.7726974, 0.86186564, 0.8380048, 0.6996503,...   \n",
       "1064  [[0.8027966, 0.9662601, 0.93856305, 0.934313, ...   \n",
       "\n",
       "                                          spec_contrast  \\\n",
       "195   [[18.698652440717126, 18.698652440717126, 18.6...   \n",
       "1048  [[14.30063620727286, 14.30063620727286, 14.300...   \n",
       "532   [[27.160881999404914, 10.240644242524503, 17.3...   \n",
       "1273  [[25.352601414654497, 17.07070284635677, 10.08...   \n",
       "1064  [[20.23687827597825, 13.043456325271165, 21.71...   \n",
       "\n",
       "                                                tonnetz  \\\n",
       "195   [[-0.24351785481696392, -0.22269760507170488, ...   \n",
       "1048  [[0.011945099331460431, 0.02484843939140515, 0...   \n",
       "532   [[-0.039178584692475336, -0.07856553551372042,...   \n",
       "1273  [[-0.09442284617849674, -0.08483387511976943, ...   \n",
       "1064  [[0.15901379770687685, 0.10643275780873687, 0....   \n",
       "\n",
       "                      filename  \\\n",
       "195   03-01-02-01-02-02-20.wav   \n",
       "1048  03-01-06-01-01-01-14.wav   \n",
       "532   03-01-07-01-02-01-04.wav   \n",
       "1273  03-01-05-01-02-01-06.wav   \n",
       "1064  03-01-04-02-02-01-14.wav   \n",
       "\n",
       "                                                mel_pad  \\\n",
       "195   [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1048  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "532   [[5.976570963388966e-14, 3.282531801929167e-09...   \n",
       "1273  [[5.16172212883248e-06, 1.971159463209915e-06,...   \n",
       "1064  [[2.1212953268956447e-11, 1.1371188712860913e-...   \n",
       "\n",
       "                                               mfcc_pad  \\\n",
       "195   [[-976.1175537109375, -976.1175537109375, -976...   \n",
       "1048  [[-919.5419311523438, -919.5419311523438, -919...   \n",
       "532   [[-735.2310791015625, -735.2310791015625, -735...   \n",
       "1273  [[-760.2454223632812, -761.3556518554688, -763...   \n",
       "1064  [[-766.1008911132812, -766.1008911132812, -766...   \n",
       "\n",
       "                                         chromagram_pad  \\\n",
       "195   [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1048  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "532   [[0.9199958443641663, 0.941920816898346, 0.971...   \n",
       "1273  [[0.772697389125824, 0.8618656396865845, 0.838...   \n",
       "1064  [[0.8027966022491455, 0.9662600755691528, 0.93...   \n",
       "\n",
       "                                      spec_contrast_pad  \\\n",
       "195   [[18.698652440717126, 18.698652440717126, 18.6...   \n",
       "1048  [[14.30063620727286, 14.30063620727286, 14.300...   \n",
       "532   [[27.160881999404914, 10.240644242524503, 17.3...   \n",
       "1273  [[25.352601414654497, 17.07070284635677, 10.08...   \n",
       "1064  [[20.23687827597825, 13.043456325271165, 21.71...   \n",
       "\n",
       "                                            tonnetz_pad  \n",
       "195   [[-0.24351785481696392, -0.22269760507170488, ...  \n",
       "1048  [[0.011945099331460431, 0.02484843939140515, 0...  \n",
       "532   [[-0.039178584692475336, -0.07856553551372042,...  \n",
       "1273  [[-0.09442284617849674, -0.08483387511976943, ...  \n",
       "1064  [[0.15901379770687685, 0.10643275780873687, 0....  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 250)\n",
      "(6, 250)\n",
      "(20, 250)\n",
      "(12, 250)\n",
      "(7, 250)\n",
      "(384, 384)\n",
      "1152\n"
     ]
    }
   ],
   "source": [
    "# 256 + 6 + 20 + 12 + 7 = 256 + 45 = 301\n",
    "print(train['mel_pad'][0].shape)\n",
    "print(train['tonnetz_pad'][0].shape)\n",
    "print(train['mfcc_pad'][0].shape)\n",
    "print(train['chromagram_pad'][0].shape)\n",
    "print(train['spec_contrast_pad'][0].shape)\n",
    "print(model.image_size)\n",
    "print(len(train['mel_pad']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# implementing a mel-only version\n",
    "class TransformerDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.labels = df['emotion'].reset_index(drop=True)\n",
    "        self.num_labels = self.labels.nunique()\n",
    "        self.mels = df['mel_pad'].reset_index(drop=True)\n",
    "        self.max_len = self.mels[0].shape[1]\n",
    "        self.transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                             transforms.CenterCrop((self.max_len, self.max_len)),\n",
    "                                             transforms.Resize(model.image_size)])\n",
    "                                             #transforms.Normalize([0.5, 0.5, 0.5],\n",
    "                                                                  #[0.5, 0.5, 0.5]),])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.mels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        mel = self.mels.iloc[idx]\n",
    "        label = self.labels.iloc[idx]        \n",
    "        mel = self.transform(mel)\n",
    "        # stack the same mel spectrogram three times to emulate RGB image\n",
    "        mel = torch.stack([mel]*3, dim=1).squeeze(dim=0).type(torch.float)\n",
    "        label = torch.tensor(label-1).type(torch.long)\n",
    "        \n",
    "        return mel, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1152\n",
      "torch.Size([3, 384, 384])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TransformerDataset(train)\n",
    "print(len(train_dataset))\n",
    "print(train_dataset[0][0].shape)\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "img = train_dataset[0][0]\n",
    "with torch.no_grad():\n",
    "    outputs = model(img.float().unsqueeze(0)).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.9097e-01, -1.0720e-02,  2.4557e-01,  6.7795e-01,  1.5116e-01,\n",
      "         4.6209e-01, -2.9006e-02,  7.3016e-01,  1.1163e+00,  7.0787e-01,\n",
      "        -4.3001e-01, -1.3071e-01,  5.0523e-01,  1.6770e-01, -4.5211e-01,\n",
      "         7.3391e-01, -4.4808e-01, -5.5204e-01,  5.5323e-01, -6.4697e-02,\n",
      "         5.8295e-01,  8.2199e-01, -2.1265e-01,  8.8231e-01,  1.2530e+00,\n",
      "        -4.9854e-02,  4.3723e-01,  1.2918e-01,  2.4161e-02,  4.9454e-01,\n",
      "         4.6028e-01,  1.4852e-01,  9.1258e-01,  4.9916e-01,  7.5659e-01,\n",
      "         8.9623e-01,  7.1845e-01,  5.1135e-01,  5.3471e-01,  1.3954e-01,\n",
      "        -6.0740e-01,  1.5229e-01,  1.6601e-01,  5.0107e-01,  1.1624e-03,\n",
      "         3.3334e-01,  3.4759e-02,  7.6536e-01,  4.2831e-02,  3.4737e-01,\n",
      "         3.7153e-01,  2.2112e-02,  4.5110e-01,  4.6891e-01,  4.1397e-01,\n",
      "        -2.2622e-02,  1.0933e-01, -2.3489e-01,  2.7681e-02, -5.2923e-02,\n",
      "         1.3036e+00, -4.2235e-02,  2.0049e-01, -6.9411e-02,  2.6302e-01,\n",
      "        -3.3694e-01,  5.5122e-01,  2.4709e-01,  5.1895e-01,  1.3617e-03,\n",
      "        -2.2619e-01,  9.6184e-01,  5.1865e-01,  9.6305e-01,  6.9484e-02,\n",
      "        -5.0899e-02,  1.6395e-01,  1.8628e-01,  8.2778e-01,  5.9536e-01,\n",
      "         1.7789e+00,  7.4567e-01,  1.5682e+00,  1.1718e+00, -1.4440e-01,\n",
      "         3.7520e-01,  1.2006e+00, -7.7947e-02, -3.5718e-01,  3.4227e-01,\n",
      "        -2.8469e-02,  2.5263e-01, -8.0478e-02,  4.2527e-01, -2.5677e-01,\n",
      "         3.5433e-01,  5.4993e-01,  1.1597e+00,  6.5673e-01,  9.0816e-01,\n",
      "         5.4574e-01,  6.4177e-01, -6.0775e-01, -1.2993e-01, -7.0719e-01,\n",
      "        -3.0100e-02,  4.6640e-01, -1.7799e-01, -8.5808e-02,  1.7477e-02,\n",
      "         1.9553e-01,  1.1511e+00, -3.3517e-01, -7.7415e-02,  3.0390e-01,\n",
      "        -2.6035e-02,  2.1640e-01,  2.1039e-01,  3.3296e-02,  2.6578e-01,\n",
      "         1.7192e-02,  7.7017e-01, -3.3789e-02, -1.4595e-01, -3.8151e-02,\n",
      "        -2.5033e-01,  5.5315e-01,  4.6477e-01,  1.0805e+00,  5.1101e-01,\n",
      "         7.6891e-01, -8.8662e-02,  2.4856e-01,  4.0567e-01,  7.4931e-01,\n",
      "         2.3563e-01,  7.6178e-01,  6.7803e-01,  6.9641e-01, -8.7083e-02,\n",
      "        -2.2139e-01,  1.6359e-01, -5.3652e-01, -6.8608e-02, -1.0542e-01,\n",
      "         4.2957e-01,  1.0332e+00,  4.5569e-02, -1.3171e-01,  5.0957e-01,\n",
      "        -2.4505e-01, -9.1654e-01, -9.0937e-02, -6.2445e-01, -8.8219e-01,\n",
      "        -4.0422e-01, -8.0876e-01, -5.5189e-01, -3.0103e-01, -2.0267e-01,\n",
      "         1.6972e-02, -1.3520e-01, -5.9655e-01, -6.6290e-02,  3.6971e-01,\n",
      "         3.5623e-01,  3.4952e-01,  2.0266e-01,  1.8841e-03, -5.3618e-01,\n",
      "        -9.8311e-02, -4.7018e-01, -3.7775e-01, -4.1919e-01, -1.1433e+00,\n",
      "         6.6093e-01, -4.3845e-01, -1.1753e-01, -5.2766e-01,  1.4031e-01,\n",
      "        -2.7152e-01,  5.8147e-02, -1.6775e-01, -2.9594e-01,  6.3470e-01,\n",
      "         2.4207e-01,  2.9566e-01,  5.2372e-02,  4.4494e-01,  1.5174e-01,\n",
      "         3.1652e-01,  1.5546e-01, -4.2223e-01,  1.2555e-01,  1.2545e-01,\n",
      "        -5.8656e-01,  9.2172e-02,  1.7398e-01, -3.5415e-01,  1.3814e-04,\n",
      "         3.3146e-01, -2.2053e-01, -3.7024e-02, -3.0429e-01, -1.1635e+00,\n",
      "        -2.2846e-01, -3.2558e-01, -5.2651e-01, -3.8726e-01, -1.4438e-02,\n",
      "        -2.7153e-01, -2.2507e-02,  4.9992e-02,  1.9697e-01,  5.1518e-01,\n",
      "        -7.5475e-01,  2.5038e-01,  3.2818e-01, -4.1004e-01, -5.3245e-01,\n",
      "         5.6922e-01, -9.4297e-02, -4.1938e-01, -2.7336e-01,  2.8103e-01,\n",
      "        -7.1171e-01,  1.9943e-01,  2.0997e-01, -6.9983e-01, -2.9021e-01,\n",
      "        -5.4513e-01,  5.1352e-01, -6.5696e-02,  1.0286e-01,  2.7864e-01,\n",
      "        -1.2873e+00,  4.5364e-03, -3.6951e-01, -4.3009e-01, -5.3248e-01,\n",
      "         2.7212e-01, -3.9223e-01, -5.9811e-01, -2.9712e-01, -2.3474e-01,\n",
      "        -4.2572e-01, -3.1368e-01,  2.5160e-02, -4.5786e-02, -6.5160e-01,\n",
      "        -2.7385e-01, -5.5693e-02, -1.6977e-02, -1.3609e-02, -8.2926e-01,\n",
      "        -3.5817e-01,  1.5111e-01, -5.8633e-01, -1.0135e-01, -5.2973e-01,\n",
      "        -5.7920e-01, -6.5186e-01, -3.4066e-01, -7.0610e-01, -2.9614e-01,\n",
      "        -5.3138e-01, -4.0602e-01,  1.1526e-01, -1.1617e+00, -2.1573e-02,\n",
      "         4.5556e-01,  7.1466e-01,  1.6669e-01, -3.5650e-01,  4.6898e-02,\n",
      "        -9.7231e-02, -1.7757e-02, -3.8176e-02, -1.6881e-01,  3.4075e-02,\n",
      "         6.8227e-01, -8.4783e-01, -7.6226e-02,  8.9084e-02, -1.2481e-01,\n",
      "        -3.6017e-01, -3.4162e-01,  5.6800e-01,  2.4417e-01,  1.5987e-01,\n",
      "         3.3161e-01,  1.8975e-01, -3.6400e-01, -8.8288e-02,  1.1242e+00,\n",
      "         9.8142e-01,  5.5936e-01,  1.1593e+00,  9.0838e-01,  1.0217e+00,\n",
      "        -3.1386e-02, -1.7866e-01,  7.5237e-01,  1.1803e-01, -1.5104e-01,\n",
      "         1.5893e-01, -1.6055e-01, -5.4426e-01, -1.3553e-01, -1.6649e-01,\n",
      "         1.9629e-01,  8.3563e-02,  1.5073e-01,  1.1703e-01,  8.7626e-01,\n",
      "         9.1888e-02,  2.2520e-01,  2.9127e-01,  5.3760e-01,  1.8567e-01,\n",
      "         3.8008e-01,  3.8093e-01,  1.5314e-01,  2.6740e-01,  3.7579e-01,\n",
      "         2.0699e-01, -2.4848e-01, -1.6465e-01, -1.2863e-01, -1.6126e-01,\n",
      "         2.6490e-01,  1.1924e-01, -2.0547e-01, -2.0248e-01,  3.9146e-01,\n",
      "         8.4475e-01,  1.0273e+00,  1.4718e+00, -3.2796e-01,  6.4117e-02,\n",
      "        -1.7868e-01,  6.3729e-01,  5.1524e-01, -9.2524e-02,  2.5614e-01,\n",
      "         5.8624e-01, -5.8598e-01, -2.0120e-01, -4.3619e-01,  7.9798e-02,\n",
      "         3.8363e-02,  3.8223e-01,  6.1417e-02,  2.1856e-01,  2.4577e-01,\n",
      "         1.3384e-01,  1.2642e+00,  1.3499e+00,  7.6558e-01,  7.0655e-01,\n",
      "         6.1809e-01,  7.2832e-01,  1.0872e+00,  6.1779e-01,  1.0648e+00,\n",
      "         2.6696e-01,  9.8576e-01,  6.9134e-01,  6.9362e-01,  6.5892e-01,\n",
      "         7.4921e-01,  4.4343e-01,  6.8739e-02, -1.9427e-01,  1.5228e-01,\n",
      "         2.6204e-01,  3.6534e-01,  7.3210e-01,  1.7631e-01,  1.3308e-01,\n",
      "         1.2551e+00,  7.3976e-01, -8.7938e-02,  2.3970e-01,  1.1801e-01,\n",
      "        -3.8711e-02, -1.9928e-01,  3.7690e-01,  3.4252e-01,  7.0964e-01,\n",
      "        -8.2026e-01,  6.7327e-02,  7.4013e-01, -2.1360e-01,  5.3484e-01,\n",
      "         1.3686e+00,  1.2230e-01, -3.2654e-01, -1.8717e-03, -2.3959e-01,\n",
      "        -4.0293e-01, -2.6830e-01,  6.1770e-01,  8.2175e-02, -2.4463e-01,\n",
      "         6.4439e-01, -1.4413e+00, -3.2679e-01,  2.8438e-01,  1.1735e+00,\n",
      "        -3.8482e-02, -1.4332e+00,  7.9040e-01, -1.0351e-01, -5.3734e-01,\n",
      "        -4.6609e-01, -3.0944e-01,  4.5681e-01, -6.1876e-01, -9.2794e-01,\n",
      "         5.4035e-01, -6.6926e-01,  9.6555e-03,  1.8307e-01, -1.9796e-01,\n",
      "         3.5972e-01,  6.5495e-01,  1.1552e+00,  3.1035e-01,  1.0585e+00,\n",
      "         1.4900e-01, -5.8662e-01,  1.5650e-01,  1.3667e-01, -7.1806e-01,\n",
      "         1.2509e-02, -1.9750e-01, -1.4854e-01,  4.7385e-01,  3.4326e-01,\n",
      "        -4.9618e-01, -2.0694e-01,  4.7583e-01, -4.1505e-01, -5.2255e-01,\n",
      "        -1.0286e-01, -6.6836e-01, -7.5090e-01,  5.6608e-02, -1.2743e-01,\n",
      "         1.7347e-01, -8.6776e-02, -3.3288e-01, -3.8436e-01, -1.3447e+00,\n",
      "         2.5560e-01,  9.5841e-01, -6.0031e-01, -7.2121e-01,  1.6339e-01,\n",
      "        -4.8652e-01,  8.0230e-01,  2.2212e-02,  6.8810e-01,  1.8270e-01,\n",
      "         3.3192e-02,  2.9322e-01, -4.0227e-01, -9.9386e-02, -5.6126e-01,\n",
      "         3.3534e-01, -2.4024e-01,  7.2390e-02, -1.2305e+00, -6.9782e-01,\n",
      "         1.1744e-01,  1.0240e+00, -2.1204e-01, -8.3213e-01,  1.5866e-01,\n",
      "        -7.4833e-01,  1.0045e+00,  3.6136e-01, -4.3703e-01,  1.0662e-01,\n",
      "         6.0589e-02,  8.0449e-01, -5.8908e-01,  1.2394e+00,  3.6968e-01,\n",
      "         9.0697e-01,  2.0161e-01, -3.6283e-01,  5.1568e-01,  7.1257e-01,\n",
      "        -3.2322e-01, -7.0160e-01, -4.8242e-01, -2.8388e-01, -7.6670e-01,\n",
      "        -5.9439e-01,  3.5809e-02, -3.6879e-01, -3.7697e-01,  2.0163e-01,\n",
      "         2.8385e-01,  1.2798e-01, -4.7329e-01,  1.4191e-01, -4.7028e-01,\n",
      "        -6.7229e-02, -8.8657e-01, -5.7359e-01, -8.7620e-02, -1.4438e-01,\n",
      "        -3.5558e-01, -8.1170e-01,  1.8970e-01,  2.7869e-01,  8.3753e-01,\n",
      "        -7.4490e-01, -1.2515e-01,  6.6521e-01, -5.7605e-01,  3.1592e-01,\n",
      "        -7.5991e-01, -2.9442e-01,  6.0384e-02, -1.9509e-01, -9.9493e-01,\n",
      "         7.2693e-01,  2.5178e-01, -8.4588e-01,  5.6282e-05, -6.6361e-02,\n",
      "         5.0479e-02,  3.8916e-01,  2.5066e-02, -1.9433e-01,  9.8937e-02,\n",
      "         5.8965e-01,  5.3921e-01,  5.3904e-01, -6.3494e-01, -1.3689e+00,\n",
      "        -2.0784e-01, -3.1145e-01, -2.3463e-01, -3.1063e-02, -9.2729e-01,\n",
      "        -3.2515e-01, -9.6703e-01, -5.5230e-01,  3.4128e-01, -5.0717e-01,\n",
      "        -5.1319e-01, -5.3814e-02, -1.4645e-02,  3.7743e-01,  4.4804e-01,\n",
      "         3.9782e-01,  5.1017e-01, -4.5591e-01, -6.9421e-01, -4.4462e-01,\n",
      "         1.1255e-01, -1.0680e+00, -1.0139e+00, -5.2669e-01, -7.6577e-01,\n",
      "         3.4577e-02, -6.7088e-01,  1.0008e-01,  5.1143e-01,  9.7712e-01,\n",
      "         2.9170e-01,  4.4323e-01,  7.6290e-02, -2.8691e-01, -8.9766e-01,\n",
      "        -1.6305e-01, -8.0888e-01,  1.2319e-01,  4.7188e-01,  2.7415e-01,\n",
      "        -2.9552e-01, -1.5029e-01,  1.8653e-01, -7.0719e-01, -1.3152e-01,\n",
      "        -1.8221e-01, -1.1865e+00, -5.9005e-01, -4.6252e-01,  3.7933e-01,\n",
      "         7.4041e-01,  1.9095e-01, -1.9541e-01, -2.2634e-01,  4.0675e-01,\n",
      "         9.3464e-01,  1.8742e-01,  1.3678e-01,  2.7140e-01,  4.0362e-01,\n",
      "         3.5907e-01,  8.4996e-03,  9.7202e-03, -1.6227e-02, -8.7518e-01,\n",
      "        -8.4106e-01,  2.6420e-01,  1.9797e-01,  4.0740e-01, -7.5972e-01,\n",
      "         1.1115e+00,  5.4856e-01, -3.7256e-01,  3.7697e-02, -2.4460e-01,\n",
      "        -5.4192e-01, -1.0847e-01,  1.1882e-02, -8.8434e-01, -5.2259e-01,\n",
      "         6.9985e-01, -2.0337e-02, -1.9848e-01,  4.9913e-01, -1.0361e+00,\n",
      "        -1.4310e+00, -6.7119e-01, -5.1696e-01,  7.4179e-02,  6.0701e-01,\n",
      "         1.2016e-01,  2.6804e-01, -2.7160e-01,  5.9973e-01, -6.8262e-01,\n",
      "         9.1319e-01,  1.6464e-01, -5.3824e-02,  8.2070e-01, -1.0996e-02,\n",
      "         6.1428e-01,  9.3088e-01, -2.5937e-01, -5.1509e-01,  6.5760e-01,\n",
      "        -2.7811e-02, -2.7330e-01, -7.5414e-01,  2.3862e-01, -1.0201e+00,\n",
      "        -1.2912e+00, -1.0959e+00, -2.8026e-01, -3.0221e-01, -5.7003e-01,\n",
      "         4.2517e-01, -1.0479e-01, -2.3057e-01, -9.4677e-01,  1.4209e-01,\n",
      "        -3.6632e-01, -2.9058e-01, -8.2749e-01, -2.2989e-02, -3.3056e-01,\n",
      "        -3.0413e-02,  4.9489e-02, -1.1401e+00, -1.8660e-02,  6.7677e-01,\n",
      "        -6.1247e-01,  5.2781e-01,  1.0653e+00,  7.1779e-01, -1.1124e-02,\n",
      "        -1.4711e-01, -6.0887e-01,  3.7986e-01,  3.8044e-01,  3.8052e-01,\n",
      "         5.2919e-01, -2.9391e-01,  2.2817e-02, -9.9822e-02, -5.8583e-01,\n",
      "         1.3484e-01,  6.5628e-01, -2.2552e-01,  3.5972e-01,  2.8759e-01,\n",
      "        -3.3703e-01, -9.6954e-01, -7.1501e-01,  7.5912e-02,  2.4260e-01,\n",
      "        -3.6571e-01,  2.8223e-01, -5.3643e-01,  3.4805e-01, -2.3495e-01,\n",
      "         1.1166e+00, -1.1412e+00,  1.1233e-01,  1.0793e-01, -9.2486e-01,\n",
      "        -6.2963e-01, -1.3625e+00,  4.1988e-01, -6.9090e-02,  1.1469e+00,\n",
      "         3.9482e-01, -2.4252e-01, -2.5010e-01,  2.8096e-01, -4.5611e-02,\n",
      "        -3.9533e-01, -1.1767e+00,  7.1385e-01,  1.6118e-01, -3.9581e-01,\n",
      "         6.2897e-01, -3.4484e-01, -4.6838e-01,  4.2534e-01,  2.8913e-01,\n",
      "        -1.9816e-01,  9.1536e-01, -5.3699e-02,  2.2086e+00, -1.3592e-01,\n",
      "        -3.3994e-01, -1.4549e-02, -7.4092e-01,  2.3821e-01, -6.2084e-01,\n",
      "        -1.0385e-01, -4.8206e-01, -4.1335e-01,  1.3110e-03, -3.1628e-01,\n",
      "        -8.9919e-01,  3.2017e-01, -3.7891e-01, -8.1445e-01,  3.3485e-01,\n",
      "        -7.6490e-01,  5.0400e-01,  8.7894e-01, -1.3874e-01, -1.1970e-01,\n",
      "        -2.3665e-01,  2.5968e-01, -2.6093e-01,  5.6216e-02, -1.1724e-01,\n",
      "        -1.2164e+00, -6.7974e-01, -7.6474e-01,  4.8585e-01, -4.1527e-01,\n",
      "        -5.5667e-01,  3.2122e-02,  1.0400e-01,  8.6167e-01,  3.2622e-01,\n",
      "        -6.2642e-01, -6.7472e-01,  3.6823e-01, -1.5079e-01, -9.3874e-01,\n",
      "        -1.2575e+00, -3.1600e-01, -7.5093e-02, -6.2290e-01, -3.8872e-02,\n",
      "        -1.9828e-01,  2.9990e-01,  6.4997e-01,  6.9841e-01, -4.4694e-02,\n",
      "        -5.4861e-01, -3.4081e-01,  1.8245e-01, -8.1865e-01,  1.6448e-01,\n",
      "        -6.9995e-01, -3.1017e-02, -5.6292e-02,  9.7180e-01, -4.0394e-01,\n",
      "        -8.0470e-01,  3.9016e-01, -8.6988e-01,  4.5151e-01,  3.2852e-01,\n",
      "         5.8553e-01,  2.2239e-01, -4.0033e-02, -1.2674e-01,  3.0190e-01,\n",
      "         4.8829e-01, -4.4099e-01,  4.2036e-01,  2.9748e-01, -6.3962e-01,\n",
      "         1.0055e+00,  1.9934e-01,  2.0827e-02, -5.7306e-01, -4.0269e-02,\n",
      "        -1.6323e-01, -5.1644e-01, -3.7914e-01, -1.2584e-01, -5.6814e-01,\n",
      "        -1.0821e-01,  1.9819e-02,  5.9578e-01, -4.1142e-01, -1.2232e+00,\n",
      "        -4.6597e-01,  7.7031e-01, -6.9872e-01,  2.3172e-01, -3.9429e-02,\n",
      "         4.6789e-02, -7.6720e-01, -3.9587e-01,  7.5832e-01, -9.6528e-01,\n",
      "         2.2933e-02, -8.8756e-01, -2.1073e-01, -1.8570e+00, -6.0811e-01,\n",
      "         9.6651e-02, -4.6671e-01, -9.6444e-01,  5.2160e-01, -6.4488e-01,\n",
      "         1.4580e-01, -2.6000e-01,  8.8550e-02,  7.0539e-01, -9.7706e-02,\n",
      "        -1.3449e+00,  5.5734e-01,  1.6021e+00,  3.0141e-01, -8.5851e-01,\n",
      "         4.1227e-01,  6.9971e-01, -1.8276e-01,  3.8085e-02,  2.6605e-01,\n",
      "        -6.2496e-01,  3.1859e-01,  1.6453e-01,  9.7843e-01, -3.4428e-01,\n",
      "        -6.3695e-01,  7.5947e-01, -2.1942e-01, -2.3668e-01, -1.1204e+00,\n",
      "         9.3173e-02, -5.7455e-01, -9.5779e-02, -3.6256e-02,  4.3157e-01,\n",
      "        -4.7716e-01,  2.7007e-01, -4.1141e-01,  1.6706e-01, -4.4568e-01,\n",
      "         5.3163e-01,  1.9440e-01,  1.1520e-01,  9.7212e-02,  3.4827e-01,\n",
      "        -1.3494e+00, -6.9401e-01, -7.0139e-01,  1.0935e-01,  5.1046e-01,\n",
      "         4.0717e-01, -6.3372e-01,  4.4076e-01, -5.8511e-01, -3.0936e-01,\n",
      "         1.1561e-01, -2.3019e-01, -1.9845e-01, -5.3501e-01, -2.2912e-01,\n",
      "         1.2828e-01,  8.3576e-01, -9.0020e-02, -7.6886e-01,  4.7904e-01,\n",
      "         1.1480e-01, -5.7152e-01, -1.0090e+00, -3.9103e-01, -1.6070e+00,\n",
      "        -1.7002e-01, -2.0276e-02,  1.5287e+00,  2.7024e-01, -6.3275e-01,\n",
      "        -6.3180e-01,  2.0602e-01, -5.6055e-01, -2.1280e-01,  1.3570e-01,\n",
      "        -7.2898e-02, -5.0728e-01, -9.9528e-01,  4.5060e-01, -2.4503e-01,\n",
      "        -5.1035e-01, -3.9161e-01,  2.0556e-01, -4.3955e-01, -4.5394e-01,\n",
      "        -3.3511e-01, -6.2647e-01, -5.0123e-01, -2.0480e-01, -7.8319e-01,\n",
      "        -2.3379e-01, -2.6652e-01, -2.1544e-01, -5.0621e-01, -8.7527e-01,\n",
      "         4.1622e-01, -3.2624e-01, -1.0017e+00,  4.7521e-01,  3.2937e-02,\n",
      "        -1.3770e-01,  3.4245e-01, -2.5621e-01, -7.2589e-02,  6.2858e-01,\n",
      "        -9.3057e-04, -6.1605e-02, -1.7761e-01,  3.4448e-02, -1.1625e-01,\n",
      "        -4.7405e-01, -2.9208e-01,  2.6852e-01,  5.2584e-01, -5.7298e-01,\n",
      "        -5.8742e-01,  1.8392e-01, -7.9921e-03,  1.8397e-04, -3.5063e-01,\n",
      "        -3.1352e-01, -3.6719e-01,  1.9700e-01,  5.4918e-02,  1.8615e-01,\n",
      "         2.5133e-01,  8.6984e-01, -6.8955e-01, -1.7343e-02,  6.1202e-02,\n",
      "         3.4454e-01, -6.4523e-01,  9.5603e-02, -4.0417e-02, -2.4127e-01,\n",
      "        -2.8438e-01,  5.7259e-01, -3.3134e-01,  3.7495e-01,  3.9600e-01,\n",
      "        -3.7997e-02,  3.7576e-01, -9.9364e-02, -6.1967e-01,  1.3353e-01,\n",
      "        -1.4407e-01,  5.6570e-01,  1.8949e-01, -5.0925e-01,  5.9867e-01,\n",
      "         9.7902e-01,  5.2102e-01, -6.1411e-01,  3.1942e-01,  4.6820e-01])\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## START BUILDING CLASSIFICATION MODEL ON TOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights.\n"
     ]
    }
   ],
   "source": [
    "model_ft = ViT(model_name, pretrained=True)\n",
    "set_parameter_requires_grad(model_ft, feature_extracting=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = torch.nn.Linear(num_ftrs, train_dataset.num_labels)\n",
    "input_size = 384\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "\n",
    "# TODO: try unfreezing the last few layers as well\n",
    "print(\"Params to learn:\")\n",
    "params_to_update = []\n",
    "for name,param in model_ft.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\",name)\n",
    "\n",
    "\n",
    "# Try the original transformer's optimizer\n",
    "optimizer_ft = transformers.AdamW(params_to_update, lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import copy\n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 1.9291 Acc: 0.2561\n",
      "val Loss: 1.8176 Acc: 0.2986\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-edd640992cd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Train and evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_inception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"inception\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-c294d2957ad0>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders, criterion, optimizer, num_epochs, is_inception)\u001b[0m\n\u001b[1;32m     46\u001b[0m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs224s-final/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs224s-final/lib/python3.6/site-packages/pytorch_pretrained_vit/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'positional_embedding'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# b,gh*gw+1,d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# b,gh*gw+1,d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pre_logits'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs224s-final/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs224s-final/lib/python3.6/site-packages/pytorch_pretrained_vit/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs224s-final/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs224s-final/lib/python3.6/site-packages/pytorch_pretrained_vit/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpwff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs224s-final/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs224s-final/lib/python3.6/site-packages/pytorch_pretrained_vit/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# (B, S, D) -> (B, S, D_ff) -> (B, S, D)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs224s-final/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mgelu\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m   1457\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_unary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1458\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1459\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "train_dataset = TransformerDataset(train)\n",
    "val_dataset = TransformerDataset(val)\n",
    "test_dataset = TransformerDataset(test)\n",
    "dataloaders_dict = {'train': torch.utils.data.DataLoader(train_dataset), \n",
    "                    'val': torch.utils.data.DataLoader(val_dataset), \n",
    "                    'test': torch.utils.data.DataLoader(test_dataset)}\n",
    "# Train and evaluate\n",
    "num_epochs = 15\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft, \"visual_transformer_mel_cropped_stacked_freeze_layers_lr1e-9_Adam.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ohist = [h.cpu().numpy() for h in hist]\n",
    "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
